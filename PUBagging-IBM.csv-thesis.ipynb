{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b722b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, auc \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, average_precision_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import igraph as ig\n",
    "from sklearn.base import clone\n",
    "import time\n",
    "from __future__ import division\n",
    "import itertools\n",
    "import numbers\n",
    "from warnings import warn\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import ClassifierMixin\n",
    "from six import with_metaclass\n",
    "from six.moves import zip\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import (check_random_state, check_X_y, check_array, column_or_1d,)\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.utils.validation import has_fit_parameter, check_is_fitted\n",
    "from sklearn.utils import indices_to_mask, check_consistent_length\n",
    "from sklearn.utils.metaestimators import if_delegate_has_method\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.ensemble import BaseEnsemble\n",
    "from sklearn.ensemble._base import _partition_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62185a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "data = pd.read_csv(\"HI-Small_trans_subset.csv\") \n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e26a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data['Hour'] = data['Timestamp'].dt.hour\n",
    "data['Date_Year'] = data['Timestamp'].dt.year\n",
    "data['Date_Month'] = data['Timestamp'].dt.month\n",
    "data['Date_Day'] = data['Timestamp'].dt.day\n",
    "\n",
    "data.drop(columns=['Timestamp'], inplace=True)\n",
    "\n",
    "# Combine Amount Paid and Amount Received to one column\n",
    "data['Amount'] = data[['Amount Paid', 'Amount Received']].mean(axis=1)  # of kies alleen één\n",
    "# Account column as string\n",
    "data['Account'] = data['Account'].astype(str)\n",
    "data['Account.1'] = data['Account.1'].astype(str)\n",
    "# Remove rows with no Account \n",
    "data.dropna(subset=['Account', 'Account.1'], inplace=True)\n",
    "\n",
    "target = data['Is Laundering']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78825620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built a graph\n",
    "G = nx.DiGraph()\n",
    "edges = list(zip(data[\"Account\"], data[\"Account.1\"], data[\"Amount\"]))\n",
    "G.add_weighted_edges_from(edges)\n",
    "print(f\"Aantal nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Aantal edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Convert to igraph\n",
    "G_ig = ig.Graph.TupleList(list(zip(data[\"Account\"], data[\"Account.1\"])), directed=True)\n",
    "\n",
    "# calculate the network features\n",
    "data[\"degree_centrality\"] = data[\"Account\"].map(dict(zip(G_ig.vs[\"name\"], G_ig.degree()))).fillna(0)\n",
    "pagerank_scores = G_ig.pagerank()\n",
    "data[\"pagerank\"] = data[\"Account\"].map(dict(zip(G_ig.vs[\"name\"], pagerank_scores))).fillna(0)\n",
    "\n",
    "# time feature\n",
    "window_size = 50\n",
    "data[\"rolling_24h_amount\"] = data.groupby(\"Account\")[\"Amount\"].rolling(window_size).sum().reset_index(0, drop=True).fillna(0)\n",
    "\n",
    "# transacation count \n",
    "data[\"transaction_count\"] = data.groupby(\"Account\")[\"Amount\"].transform(\"count\")\n",
    "\n",
    "#features\n",
    "features = data.drop(columns=[\n",
    "    'Is Laundering',\n",
    "    'Payment Currency',\n",
    "    'Receiving Currency'\n",
    "]).copy()\n",
    "\n",
    "features[\"degree_centrality\"] = data[\"degree_centrality\"]\n",
    "features[\"pagerank\"] = data[\"pagerank\"]\n",
    "features[\"rolling_24h_amount\"] = data[\"rolling_24h_amount\"]\n",
    "features[\"transaction_count\"] = data[\"transaction_count\"]\n",
    "\n",
    "# Optioneel: remove the most useless features if they exist \n",
    "features.drop(columns=['Date_Year', 'Date_Month'], inplace=True, errors='ignore')\n",
    "\n",
    "# Combine features + target\n",
    "data_standardized = pd.DataFrame(features, columns=features.columns)\n",
    "data_standardized['Is Laundering'] = target.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split features and target\n",
    "X = data_standardized.drop(columns=[\"Is Laundering\"])\n",
    "y = data_standardized[\"Is Laundering\"]\n",
    "\n",
    "# 3. Train/test split with stratify\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "# Further splitting the train set into Train and Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de8f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'Account', 'Account.1',\n",
    "    'From Bank', 'To Bank',\n",
    "    'Payment Format']\n",
    "numerical_cols = ['Hour', 'Date_Day', 'Amount','degree_centrality','pagerank','rolling_24h_amount','transaction_count']\n",
    "\n",
    "# Combine train, val, and test to fit LabelEncoder\n",
    "combined_data = pd.concat([X_train, X_val, X_test])\n",
    "\n",
    "# Applying Label Encoding to Categorical Columns (Train, Val, Test)\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(combined_data[col])  \n",
    "    label_encoders[col] = encoder  \n",
    "\n",
    "    # Transform the columns\n",
    "    X_train[col] = encoder.transform(X_train[col])\n",
    "    X_val[col] = encoder.transform(X_val[col])\n",
    "    X_test[col] = encoder.transform(X_test[col])\n",
    "\n",
    "# Applying Standard Scaling to Numerical Columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numerical_cols])  # Fit only on training data\n",
    "\n",
    "# Transform the columns\n",
    "X_train[numerical_cols] = scaler.transform(X_train[numerical_cols])\n",
    "X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the PU dataset \n",
    "def maak_pu_setting_van_echte_labels(y_true, label_ratio, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    positieve_indexen = np.where(y_true == 1)[0]\n",
    "    n_gelabeld = int(label_ratio * len(positieve_indexen))\n",
    "    gelabelde_indexen = np.random.choice(positieve_indexen, size=n_gelabeld, replace=False)\n",
    "\n",
    "    y_pu = np.zeros_like(y_true)\n",
    "    y_pu[gelabelde_indexen] = 1\n",
    "\n",
    "    return y_pu, y_true, gelabelde_indexen\n",
    "alpha = 0.001  # 0.1% prior of the complete dataset\n",
    "label_ratio = 0.2  # 20% labeling probability (c)\n",
    "\n",
    "y_train_pu, y_train_true, gelabelde_indexen = maak_pu_setting_van_echte_labels(y_train, label_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bagging meta-estimator for PU learning.\n",
    "Any scikit-learn estimator should work as the base estimator.\n",
    "This implementation is fully compatible with scikit-learn, and is in fact based\n",
    "on the code of the sklearn.ensemble.BaggingClassifier class with very minor\n",
    "changes.\n",
    "\"\"\"\n",
    "\n",
    "# Author: Gilles Louppe <g.louppe@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "#\n",
    "#\n",
    "# Adapted for PU learning by Roy Wright <roy.w.wright@gmail.com>\n",
    "# (work in progress)\n",
    "#\n",
    "# A better idea: instead of a separate PU class, modify the original\n",
    "# sklearn BaggingClassifier so that the parameters max_samples\n",
    "# and bootstrap may be lists or dicts...\n",
    "# e.g. for a PU problem with 500 positives and 10000 unlabeled, we might set\n",
    "# max_samples = [500, 500]     (to balance P and U in each bag)\n",
    "# bootstrap = [True, False]    (to only bootstrap the unlabeled)\n",
    "\n",
    "__all__ = [\"BaggingPuClassifier\"]\n",
    "\n",
    "MAX_INT = np.iinfo(np.int32).max\n",
    "\n",
    "\n",
    "def _generate_indices(random_state, bootstrap, n_population, n_samples):\n",
    "    \"\"\"Draw randomly sampled indices.\"\"\"\n",
    "    # Draw sample indices\n",
    "    if bootstrap:\n",
    "        indices = random_state.randint(0, n_population, n_samples)\n",
    "    else:\n",
    "        indices = sample_without_replacement(n_population, n_samples,\n",
    "                                             random_state=random_state)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def _generate_bagging_indices(random_state, bootstrap_features,\n",
    "                              bootstrap_samples, n_features, n_samples,\n",
    "                              max_features, max_samples):\n",
    "    \"\"\"Randomly draw feature and sample indices.\"\"\"\n",
    "    # Get valid random state\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    # Draw indices\n",
    "    feature_indices = _generate_indices(random_state, bootstrap_features,\n",
    "                                        n_features, max_features)\n",
    "    sample_indices = _generate_indices(random_state, bootstrap_samples,\n",
    "                                       n_samples, max_samples)\n",
    "\n",
    "    return feature_indices, sample_indices\n",
    "\n",
    "\n",
    "def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,\n",
    "                               seeds, total_n_estimators, verbose):\n",
    "    \"\"\"Private function used to build a batch of estimators within a job.\"\"\"\n",
    "    # Retrieve settings\n",
    "    n_samples, n_features = X.shape\n",
    "    max_features = ensemble._max_features\n",
    "    max_samples = ensemble._max_samples\n",
    "    bootstrap = ensemble.bootstrap\n",
    "    bootstrap_features = ensemble.bootstrap_features\n",
    "    support_sample_weight = has_fit_parameter(ensemble.base_estimator_,\n",
    "                                              \"sample_weight\")\n",
    "    if not support_sample_weight and sample_weight is not None:\n",
    "        raise ValueError(\"The base estimator doesn't support sample weight\")\n",
    "\n",
    "    # Build estimators\n",
    "    estimators = []\n",
    "    estimators_features = []\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        if verbose > 1:\n",
    "            print(\"Building estimator %d of %d for this parallel run \"\n",
    "                  \"(total %d)...\" % (i + 1, n_estimators, total_n_estimators))\n",
    "\n",
    "        random_state = np.random.RandomState(seeds[i])\n",
    "        estimator = ensemble._make_estimator(append=False,\n",
    "                                             random_state=random_state)\n",
    "\n",
    "        # ============ MAIN MODIFICATION FOR PU LEARNING =============\n",
    "        iP = [pair[0] for pair in enumerate(y) if pair[1] == 1]\n",
    "        iU = [pair[0] for pair in enumerate(y) if pair[1] < 1]\n",
    "        features, indices = _generate_bagging_indices(random_state,\n",
    "                                                      bootstrap_features,\n",
    "                                                      bootstrap, n_features,\n",
    "                                                      len(iU), max_features,\n",
    "                                                      max_samples)\n",
    "        indices = [iU[i] for i in indices] + iP\n",
    "        # ============================================================\n",
    "\n",
    "        # Draw samples, using sample weights, and then fit\n",
    "        if support_sample_weight:\n",
    "            if sample_weight is None:\n",
    "                curr_sample_weight = np.ones((n_samples,))\n",
    "            else:\n",
    "                curr_sample_weight = sample_weight.copy()\n",
    "\n",
    "            if bootstrap:\n",
    "                sample_counts = np.bincount(indices, minlength=n_samples)\n",
    "                curr_sample_weight *= sample_counts\n",
    "            else:\n",
    "                not_indices_mask = ~indices_to_mask(indices, n_samples)\n",
    "                curr_sample_weight[not_indices_mask] = 0\n",
    "\n",
    "            estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)\n",
    "\n",
    "        # Draw samples, using a mask, and then fit\n",
    "        else:\n",
    "            estimator.fit((X[indices])[:, features], y[indices])\n",
    "\n",
    "        estimators.append(estimator)\n",
    "        estimators_features.append(features)\n",
    "\n",
    "    return estimators, estimators_features\n",
    "\n",
    "\n",
    "def _parallel_predict_proba(estimators, estimators_features, X, n_classes):\n",
    "    \"\"\"Private function used to compute (proba-)predictions within a job.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    proba = np.zeros((n_samples, n_classes))\n",
    "\n",
    "    for estimator, features in zip(estimators, estimators_features):\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            proba_estimator = estimator.predict_proba(X[:, features])\n",
    "\n",
    "            if n_classes == len(estimator.classes_):\n",
    "                proba += proba_estimator\n",
    "\n",
    "            else:  # pragma: no cover\n",
    "                proba[:, estimator.classes_] += \\\n",
    "                    proba_estimator[:, range(len(estimator.classes_))]\n",
    "\n",
    "        else:\n",
    "            # Resort to voting\n",
    "            predictions = estimator.predict(X[:, features])\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                proba[i, predictions[i]] += 1\n",
    "\n",
    "    return proba\n",
    "\n",
    "\n",
    "def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n",
    "    \"\"\"Private function used to compute log probabilities within a job.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    log_proba = np.empty((n_samples, n_classes))\n",
    "    log_proba.fill(-np.inf)\n",
    "    all_classes = np.arange(n_classes, dtype=np.int)\n",
    "\n",
    "    for estimator, features in zip(estimators, estimators_features):\n",
    "        log_proba_estimator = estimator.predict_log_proba(X[:, features])\n",
    "\n",
    "        if n_classes == len(estimator.classes_):\n",
    "            log_proba = np.logaddexp(log_proba, log_proba_estimator)\n",
    "\n",
    "        else:  # pragma: no cover\n",
    "            log_proba[:, estimator.classes_] = np.logaddexp(\n",
    "                log_proba[:, estimator.classes_],\n",
    "                log_proba_estimator[:, range(len(estimator.classes_))])\n",
    "\n",
    "            missing = np.setdiff1d(all_classes, estimator.classes_)\n",
    "            log_proba[:, missing] = np.logaddexp(log_proba[:, missing],\n",
    "                                                 -np.inf)\n",
    "\n",
    "    return log_proba\n",
    "\n",
    "\n",
    "def _parallel_decision_function(estimators, estimators_features, X):\n",
    "    \"\"\"Private function used to compute decisions within a job.\"\"\"\n",
    "    return sum(estimator.decision_function(X[:, features])\n",
    "               for estimator, features in zip(estimators,\n",
    "                                              estimators_features))\n",
    "\n",
    "\n",
    "class BaseBaggingPU(with_metaclass(ABCMeta, BaseEnsemble)):\n",
    "    \"\"\"Base class for Bagging PU meta-estimator.\n",
    "    Warning: This class should not be used directly. Use derived classes\n",
    "    instead.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=10,\n",
    "                 max_samples=1.0,\n",
    "                 max_features=1.0,\n",
    "                 bootstrap=True,\n",
    "                 bootstrap_features=False,\n",
    "                 oob_score=True,\n",
    "                 warm_start=False,\n",
    "                 n_jobs=1,\n",
    "                 random_state=None,\n",
    "                 verbose=0):\n",
    "        super(BaseBaggingPU, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators)\n",
    "\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.bootstrap_features = bootstrap_features\n",
    "        self.oob_score = oob_score\n",
    "        self.warm_start = warm_start\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Build a Bagging ensemble of estimators from the training\n",
    "           set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values (1 for positive, 0 for unlabeled).\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Note that this is supported only if the base estimator supports\n",
    "            sample weighting.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n",
    "\n",
    "    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):\n",
    "        \"\"\"Build a Bagging ensemble of estimators from the training\n",
    "           set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values (1 for positive, 0 for unlabeled).\n",
    "        max_samples : int or float, optional (default=None)\n",
    "            Argument to use instead of self.max_samples.\n",
    "        max_depth : int, optional (default=None)\n",
    "            Override value used when constructing base estimator. Only\n",
    "            supported if the base estimator has a max_depth parameter.\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Note that this is supported only if the base estimator supports\n",
    "            sample weighting.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        self.y = y\n",
    "\n",
    "        # Convert data\n",
    "        X, y = check_X_y(X, y, ['csr', 'csc'])\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            check_consistent_length(y, sample_weight)\n",
    "\n",
    "        # Remap output\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        self._n_samples = n_samples\n",
    "        y = self._validate_y(y)\n",
    "\n",
    "        # Check parameters\n",
    "        self._validate_estimator()\n",
    "\n",
    "        if max_depth is not None:  # pragma: no cover\n",
    "            self.base_estimator_.max_depth = max_depth\n",
    "\n",
    "        # Validate max_samples\n",
    "        if max_samples is None:  # pragma: no cover\n",
    "            max_samples = self.max_samples\n",
    "        elif not isinstance(max_samples, (numbers.Integral, np.integer)):\n",
    "            max_samples = int(max_samples * sum(y < 1))\n",
    "\n",
    "        if not (0 < max_samples <= sum(y < 1)):\n",
    "            raise ValueError(\n",
    "                \"max_samples must be positive\"\n",
    "                \" and no larger than the number of unlabeled points\")\n",
    "\n",
    "        # Store validated integer row sampling value\n",
    "        self._max_samples = max_samples\n",
    "\n",
    "        # Validate max_features\n",
    "        if isinstance(self.max_features, (numbers.Integral, np.integer)):\n",
    "            max_features = self.max_features\n",
    "        else:  # float\n",
    "            max_features = int(self.max_features * self.n_features_)\n",
    "\n",
    "        if not (0 < max_features <= self.n_features_):\n",
    "            raise ValueError(\"max_features must be in (0, n_features]\")\n",
    "\n",
    "        # Store validated integer feature sampling value\n",
    "        self._max_features = max_features\n",
    "\n",
    "        # Other checks\n",
    "        if not self.bootstrap and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimation only available\"\n",
    "                             \" if bootstrap=True\")\n",
    "\n",
    "        if self.warm_start and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimate only available\"\n",
    "                             \" if warm_start=False\")\n",
    "\n",
    "        if hasattr(self, \"oob_score_\") and self.warm_start:  # pragma: no cover\n",
    "            del self.oob_score_  # pragma: no covr\n",
    "\n",
    "        if not self.warm_start or not hasattr(self, 'estimators_'):\n",
    "            # Free allocated memory, if any\n",
    "            self.estimators_ = []\n",
    "            self.estimators_features_ = []\n",
    "\n",
    "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
    "\n",
    "        if n_more_estimators < 0:  # pragma: no cover\n",
    "            raise ValueError('n_estimators=%d must be larger or equal to '\n",
    "                             'len(estimators_)=%d when warm_start==True'\n",
    "                             % (self.n_estimators, len(self.estimators_)))\n",
    "\n",
    "        if n_more_estimators == 0:\n",
    "            warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
    "                 \"fit new trees.\")\n",
    "            return self\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,\n",
    "                                                             self.n_jobs)\n",
    "        total_n_estimators = sum(n_estimators)\n",
    "\n",
    "        # Advance random state to state after training\n",
    "        # the first n_estimators\n",
    "        if self.warm_start and len(self.estimators_) > 0:  # pragma: no cover\n",
    "            random_state.randint(MAX_INT, size=len(self.estimators_))\n",
    "\n",
    "        seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n",
    "        self._seeds = seeds\n",
    "\n",
    "        all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "            delayed(_parallel_build_estimators)(\n",
    "                n_estimators[i],\n",
    "                self,\n",
    "                X,\n",
    "                y,\n",
    "                sample_weight,\n",
    "                seeds[starts[i]:starts[i + 1]],\n",
    "                total_n_estimators,\n",
    "                verbose=self.verbose)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        self.estimators_ += list(itertools.chain.from_iterable(\n",
    "            t[0] for t in all_results))\n",
    "        self.estimators_features_ += list(itertools.chain.from_iterable(\n",
    "            t[1] for t in all_results))\n",
    "\n",
    "        if self.oob_score:\n",
    "            self._set_oob_score(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def _set_oob_score(self, X, y):\n",
    "        \"\"\"Calculate out of bag predictions and score.\"\"\"\n",
    "\n",
    "    def _validate_y(self, y):  # pragma: no cover\n",
    "        # Default implementation\n",
    "        return column_or_1d(y, warn=True)\n",
    "\n",
    "    def _get_estimators_indices(self):\n",
    "        # Get drawn indices along both sample and feature axes\n",
    "        for seed in self._seeds:\n",
    "            # Operations accessing random_state must be performed identically\n",
    "            # to those in _parallel_build_estimators()\n",
    "            random_state = np.random.RandomState(seed)\n",
    "\n",
    "            # ============ MAIN MODIFICATION FOR PU LEARNING =============\n",
    "            iP = [pair[0] for pair in enumerate(self.y) if pair[1] == 1]\n",
    "            iU = [pair[0] for pair in enumerate(self.y) if pair[1] < 1]\n",
    "\n",
    "            feature_indices, sample_indices = _generate_bagging_indices(\n",
    "                random_state, self.bootstrap_features, self.bootstrap,\n",
    "                self.n_features_, len(iU), self._max_features,\n",
    "                self._max_samples)\n",
    "\n",
    "            sample_indices = [iU[i] for i in sample_indices] + iP\n",
    "            # ============================================================\n",
    "\n",
    "            yield feature_indices, sample_indices\n",
    "\n",
    "    @property\n",
    "    def estimators_samples_(self):\n",
    "        \"\"\"The subset of drawn samples for each base estimator.\n",
    "        Returns a dynamically generated list of boolean masks identifying\n",
    "        the samples used for fitting each member of the ensemble, i.e.,\n",
    "        the in-bag samples.\n",
    "        Note: the list is re-created at each call to the property in order\n",
    "        to reduce the object memory footprint by not storing the sampling\n",
    "        data. Thus fetching the property may be slower than expected.\n",
    "        \"\"\"\n",
    "        sample_masks = []\n",
    "        for _, sample_indices in self._get_estimators_indices():\n",
    "            mask = indices_to_mask(sample_indices, self._n_samples)\n",
    "            sample_masks.append(mask)\n",
    "\n",
    "        return sample_masks\n",
    "\n",
    "\n",
    "class BaggingPuClassifier(BaseBaggingPU, ClassifierMixin):\n",
    "    \"\"\"A Bagging PU classifier.\n",
    "    Adapted from sklearn.ensemble.BaggingClassifier, based on\n",
    "    A bagging SVM to learn from positive and unlabeled examples (2013)\n",
    "    by Mordelet and Vert\n",
    "    http://dx.doi.org/10.1016/j.patrec.2013.06.010\n",
    "    http://members.cbio.mines-paristech.fr/~jvert/svn/bibli/local/Mordelet2013bagging.pdf\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object or None, optional (default=None)\n",
    "        The base estimator to fit on random subsets of the dataset.\n",
    "        If None, then the base estimator is a decision tree.\n",
    "    n_estimators : int, optional (default=10)\n",
    "        The number of base estimators in the ensemble.\n",
    "    max_samples : int or float, optional (default=1.0)\n",
    "        The number of unlabeled samples to draw to train each base estimator.\n",
    "    max_features : int or float, optional (default=1.0)\n",
    "        The number of features to draw from X to train each base estimator.\n",
    "        - If int, then draw max_features features.\n",
    "        - If float, then draw max_features * X.shape[1] features.\n",
    "    bootstrap : boolean, optional (default=True)\n",
    "        Whether samples are drawn with replacement.\n",
    "    bootstrap_features : boolean, optional (default=False)\n",
    "        Whether features are drawn with replacement.\n",
    "    oob_score : bool, optional (default=True)\n",
    "        Whether to use out-of-bag samples to estimate\n",
    "        the generalization error.\n",
    "    warm_start : bool, optional (default=False)\n",
    "        When set to True, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit\n",
    "        a whole new ensemble.\n",
    "    n_jobs : int, optional (default=1)\n",
    "        The number of jobs to run in parallel for both fit and predict.\n",
    "        If -1, then the number of jobs is set to the number of cores.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    verbose : int, optional (default=0)\n",
    "        Controls the verbosity of the building process.\n",
    "    Attributes\n",
    "    ----------\n",
    "    base_estimator_ : estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "    estimators_ : list of estimators\n",
    "        The collection of fitted base estimators.\n",
    "    estimators_samples_ : list of arrays\n",
    "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
    "        estimator. Each subset is defined by a boolean mask.\n",
    "    estimators_features_ : list of arrays\n",
    "        The subset of drawn features for each base estimator.\n",
    "    classes_ : array of shape = [n_classes]\n",
    "        The classes labels.\n",
    "    n_classes_ : int or list\n",
    "        The number of classes.\n",
    "    oob_score_ : float\n",
    "        Score of the training dataset obtained using an out-of-bag estimate.\n",
    "    oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
    "        Decision function computed with out-of-bag estimate on the training\n",
    "        set. Positive data points, and perhaps some of the unlabeled,\n",
    "        are left out during the bootstrap. In these cases,\n",
    "        oob_decision_function_ contains NaN.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=10,\n",
    "                 max_samples=1.0,\n",
    "                 max_features=1.0,\n",
    "                 bootstrap=True,\n",
    "                 bootstrap_features=False,\n",
    "                 oob_score=True,\n",
    "                 warm_start=False,\n",
    "                 n_jobs=1,\n",
    "                 random_state=None,\n",
    "                 verbose=0):\n",
    "\n",
    "        super(BaggingPuClassifier, self).__init__(\n",
    "            base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            max_samples=max_samples,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            bootstrap_features=bootstrap_features,\n",
    "            oob_score=oob_score,\n",
    "            warm_start=warm_start,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose)\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
    "        super(BaggingPuClassifier, self)._validate_estimator(\n",
    "            default=DecisionTreeClassifier())\n",
    "\n",
    "    def _set_oob_score(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "        n_classes_ = self.n_classes_\n",
    "\n",
    "        predictions = np.zeros((n_samples, n_classes_))\n",
    "\n",
    "        for estimator, samples, features in zip(self.estimators_,\n",
    "                                                self.estimators_samples_,\n",
    "                                                self.estimators_features_):\n",
    "            # Create mask for OOB samples\n",
    "            mask = ~samples\n",
    "\n",
    "            if hasattr(estimator, \"predict_proba\"):\n",
    "                predictions[mask, :] += estimator.predict_proba(\n",
    "                    (X[mask, :])[:, features])\n",
    "\n",
    "            else:\n",
    "                p = estimator.predict((X[mask, :])[:, features])\n",
    "                j = 0\n",
    "\n",
    "                for i in range(n_samples):\n",
    "                    if mask[i]:\n",
    "                        predictions[i, p[j]] += 1\n",
    "                        j += 1\n",
    "\n",
    "        # Modified: no warnings about non-OOB points (i.e. positives)\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            denominator = predictions.sum(axis=1)[:, np.newaxis]\n",
    "            oob_decision_function = predictions / denominator\n",
    "            oob_score = accuracy_score(y, np.argmax(predictions, axis=1))\n",
    "\n",
    "        self.oob_decision_function_ = oob_decision_function\n",
    "        self.oob_score_ = oob_score\n",
    "\n",
    "    def _validate_y(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\n",
    "        The predicted class of an input sample is computed as the class with\n",
    "        the highest mean predicted probability. If base estimators do not\n",
    "        implement a `predict_proba method, then it resorts to voting.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        predicted_probabilitiy = self.predict_proba(X)\n",
    "        return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),\n",
    "                                  axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the mean predicted class probabilities of the base estimators in the\n",
    "        ensemble. If base estimators do not implement a `predict_proba\n",
    "        method, then it resorts to voting and the predicted class probabilities\n",
    "        of an input sample represents the proportion of estimators predicting\n",
    "        each class.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute classes_.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "        # Check data\n",
    "        X = check_array(X, accept_sparse=['csr', 'csc'])\n",
    "\n",
    "        if self.n_features_ != X.shape[1]:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is {0} and \"\n",
    "                             \"input n_features is {1}.\"\n",
    "                             \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
    "                                                             self.n_jobs)\n",
    "\n",
    "        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "            delayed(_parallel_predict_proba)(\n",
    "                self.estimators_[starts[i]:starts[i + 1]],\n",
    "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                X,\n",
    "                self.n_classes_)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        proba = sum(all_proba) / self.n_estimators\n",
    "\n",
    "        return proba\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Predict class log-probabilities for X.\n",
    "        The predicted class log-probabilities of an input sample is computed as\n",
    "        the log of the mean predicted class probabilities of the base\n",
    "        estimators in the ensemble.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class log-probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute classes_.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "        if hasattr(self.base_estimator_, \"predict_log_proba\"):\n",
    "            # Check data\n",
    "            X = check_array(X, accept_sparse=['csr', 'csc'])\n",
    "\n",
    "            if self.n_features_ != X.shape[1]:\n",
    "                raise ValueError(\"Number of features of the model must \"\n",
    "                                 \"match the input. Model n_features is {0} \"\n",
    "                                 \"and input n_features is {1} \"\n",
    "                                 \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "            # Parallel loop\n",
    "            n_jobs, n_estimators, starts = _partition_estimators(\n",
    "                self.n_estimators, self.n_jobs)\n",
    "\n",
    "            all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "                delayed(_parallel_predict_log_proba)(\n",
    "                    self.estimators_[starts[i]:starts[i + 1]],\n",
    "                    self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                    X,\n",
    "                    self.n_classes_)\n",
    "                for i in range(n_jobs))\n",
    "\n",
    "            # Reduce\n",
    "            log_proba = all_log_proba[0]\n",
    "\n",
    "            for j in range(1, len(all_log_proba)):  # pragma: no cover\n",
    "                log_proba = np.logaddexp(log_proba, all_log_proba[j])\n",
    "\n",
    "            log_proba -= np.log(self.n_estimators)\n",
    "\n",
    "            return log_proba\n",
    "        # else, the base estimator has no predict_log_proba, so...\n",
    "        return np.log(self.predict_proba(X))\n",
    "\n",
    "    @if_delegate_has_method(delegate='base_estimator')\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Average of the decision functions of the base classifiers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The columns correspond\n",
    "            to the classes in sorted order, as they appear in the attribute\n",
    "            `classes_. Regression and binary classification are special\n",
    "            cases with `k == 1, otherwise k==n_classes.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        # Check data\n",
    "        X = check_array(X, accept_sparse=['csr', 'csc'])\n",
    "\n",
    "        if self.n_features_ != X.shape[1]:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is {0} and \"\n",
    "                             \"input n_features is {1} \"\n",
    "                             \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
    "                                                             self.n_jobs)\n",
    "\n",
    "        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "            delayed(_parallel_decision_function)(\n",
    "                self.estimators_[starts[i]:starts[i + 1]],\n",
    "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                X)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        decisions = sum(all_decisions) / self.n_estimators\n",
    "\n",
    "        return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best params={'n_estimators': 80, 'max_depth': 11, 'max_samples': 590886}\n",
    "pu_clf = BaggingPuClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=11),\n",
    "    n_estimators=80,\n",
    "    max_samples=590886,        \n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on the PU version of y_train\n",
    "pu_clf.fit(X_train.values, y_train_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767dda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "y_proba = pu_clf.predict_proba(X_test)[:, 1]\n",
    "thresholds = np.linspace(0.0, 1.0, 101)\n",
    "f1_scores = [f1_score(y_test, (y_proba >= t).astype(int)) for t in thresholds]\n",
    "pr_auc_scores = [average_precision_score(y_test, (y_proba >= t).astype(int)) for t in thresholds]\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "pr_auc = average_precision_score(y_test, y_proba)\n",
    "\n",
    "print(\"Model evaluatie (PuBagging): met threshold\", best_threshold)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd20a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Not Suspicious', 'Suspicious'],\n",
    "    yticklabels=['Not Suspicious', 'Suspicious']\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ddab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna \n",
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial,X_train, y_train, X_val, y_val):\n",
    "    # Hyperparameter suggestions from Optuna\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 600)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 30)\n",
    "    max_samples = trial.suggest_int('max_samples', 500, len(X_train))\n",
    "    \n",
    "    # Initialize Bagging PU Classifier\n",
    "    pu_clf = BaggingPuClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=max_depth),\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=1.0,\n",
    "        bootstrap=True,\n",
    "        bootstrap_features=False,\n",
    "        oob_score=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train on the PU version of y_train\n",
    "    pu_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = pu_clf.predict(X_val)\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    val_score = f1_score(y_val, y_pred)\n",
    "    \n",
    "    return val_score\n",
    "\n",
    "def optimize_bagging_pu(X_train, y_train, X_val, y_val, n_trials):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=n_trials)\n",
    "    \n",
    "    print('Best Trial:')\n",
    "    print(study.best_trial)\n",
    "    \n",
    "    return study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Optuna Optimization\n",
    "best_pu_clf = optimize_bagging_pu(X_train, y_train, X_val, y_val, n_trials=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

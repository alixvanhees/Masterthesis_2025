{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e56237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-igraph in ./opt/anaconda3/lib/python3.8/site-packages (0.11.8)\n",
      "Requirement already satisfied: igraph==0.11.8 in ./opt/anaconda3/lib/python3.8/site-packages (from python-igraph) (0.11.8)\n",
      "Requirement already satisfied: texttable>=1.6.2 in ./opt/anaconda3/lib/python3.8/site-packages (from igraph==0.11.8->python-igraph) (1.7.0)\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "!pip install python-igraph\n",
    "import networkx as nx\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import igraph as ig\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from __future__ import division\n",
    "import itertools\n",
    "import numbers\n",
    "from warnings import warn\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import ClassifierMixin\n",
    "from six import with_metaclass\n",
    "from six.moves import zip\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import (check_random_state, check_X_y, check_array, column_or_1d,)\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.utils.validation import has_fit_parameter, check_is_fitted\n",
    "from sklearn.utils import indices_to_mask, check_consistent_length\n",
    "from sklearn.utils.metaestimators import if_delegate_has_method\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.ensemble import BaseEnsemble\n",
    "from sklearn.ensemble._base import _partition_estimators\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, average_precision_score, auc, classification_report \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a8dd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sender_account</th>\n",
       "      <th>Receiver_account</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Payment_currency</th>\n",
       "      <th>Received_currency</th>\n",
       "      <th>Sender_bank_location</th>\n",
       "      <th>Receiver_bank_location</th>\n",
       "      <th>Payment_type</th>\n",
       "      <th>Is_laundering</th>\n",
       "      <th>Laundering_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2999995</th>\n",
       "      <td>23:01:55</td>\n",
       "      <td>2023-06-03</td>\n",
       "      <td>6465028610</td>\n",
       "      <td>2172225510</td>\n",
       "      <td>155695.31</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>Dirham</td>\n",
       "      <td>UK</td>\n",
       "      <td>UAE</td>\n",
       "      <td>Cross-border</td>\n",
       "      <td>1</td>\n",
       "      <td>Single_large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999996</th>\n",
       "      <td>08:14:29</td>\n",
       "      <td>2023-05-10</td>\n",
       "      <td>9634559331</td>\n",
       "      <td>8020264563</td>\n",
       "      <td>2481.04</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>Cash Deposit</td>\n",
       "      <td>1</td>\n",
       "      <td>Smurfing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999997</th>\n",
       "      <td>22:05:45</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>1240988490</td>\n",
       "      <td>4597902876</td>\n",
       "      <td>3119.80</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>Yen</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>1</td>\n",
       "      <td>Structuring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999998</th>\n",
       "      <td>22:17:46</td>\n",
       "      <td>2022-10-18</td>\n",
       "      <td>6770601554</td>\n",
       "      <td>6128626942</td>\n",
       "      <td>76.65</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>Naira</td>\n",
       "      <td>UK</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>Cash Withdrawal</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash_Withdrawal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999999</th>\n",
       "      <td>05:46:53</td>\n",
       "      <td>2023-04-05</td>\n",
       "      <td>8424086459</td>\n",
       "      <td>3934928194</td>\n",
       "      <td>4284.21</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>Albanian lek</td>\n",
       "      <td>UK</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Cross-border</td>\n",
       "      <td>1</td>\n",
       "      <td>Deposit-Send</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time        Date  Sender_account  Receiver_account     Amount  \\\n",
       "2999995  23:01:55  2023-06-03      6465028610        2172225510  155695.31   \n",
       "2999996  08:14:29  2023-05-10      9634559331        8020264563    2481.04   \n",
       "2999997  22:05:45  2022-12-15      1240988490        4597902876    3119.80   \n",
       "2999998  22:17:46  2022-10-18      6770601554        6128626942      76.65   \n",
       "2999999  05:46:53  2023-04-05      8424086459        3934928194    4284.21   \n",
       "\n",
       "        Payment_currency Received_currency Sender_bank_location  \\\n",
       "2999995        UK pounds            Dirham                   UK   \n",
       "2999996        UK pounds         UK pounds                   UK   \n",
       "2999997        UK pounds               Yen                   UK   \n",
       "2999998        UK pounds             Naira                   UK   \n",
       "2999999        UK pounds      Albanian lek                   UK   \n",
       "\n",
       "        Receiver_bank_location     Payment_type  Is_laundering  \\\n",
       "2999995                    UAE     Cross-border              1   \n",
       "2999996                     UK     Cash Deposit              1   \n",
       "2999997                     UK           Cheque              1   \n",
       "2999998                Nigeria  Cash Withdrawal              1   \n",
       "2999999                Albania     Cross-border              1   \n",
       "\n",
       "         Laundering_type  \n",
       "2999995     Single_large  \n",
       "2999996         Smurfing  \n",
       "2999997      Structuring  \n",
       "2999998  Cash_Withdrawal  \n",
       "2999999     Deposit-Send  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/alixvanhees/Documents/HIRB THESIS /SAML-D_subset.csv\") \n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20fae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2996884\n",
      "1       3116\n",
      "Name: Is_laundering, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"Is_laundering\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a736d1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sender_account Receiver_account     Amount Receiver_bank_location  \\\n",
      "0           5962016906       3377238139    5285.95                     UK   \n",
      "1            985504393       8645656726    6703.57                     UK   \n",
      "2           2012195326       8080878478    2999.20                     UK   \n",
      "3           9696055743       1969723275    5844.41                     UK   \n",
      "4           9248338532       3039356124    9107.88                     UK   \n",
      "...                ...              ...        ...                    ...   \n",
      "2999995     6465028610       2172225510  155695.31                    UAE   \n",
      "2999996     9634559331       8020264563    2481.04                     UK   \n",
      "2999997     1240988490       4597902876    3119.80                     UK   \n",
      "2999998     6770601554       6128626942      76.65                Nigeria   \n",
      "2999999     8424086459       3934928194    4284.21                Albania   \n",
      "\n",
      "            Payment_type  Hour  Date_Day  degree_centrality      pagerank  \\\n",
      "0                    ACH     2         2                 87  1.522045e-06   \n",
      "1                    ACH    17        15                145  3.619998e-06   \n",
      "2           Cash Deposit     9        24                  2  8.227268e-07   \n",
      "3            Credit card    21        16                136  9.214540e-06   \n",
      "4            Credit card    16        22                156  7.116587e-06   \n",
      "...                  ...   ...       ...                ...           ...   \n",
      "2999995     Cross-border    23         3                 63  8.227268e-07   \n",
      "2999996     Cash Deposit     8        10                111  2.221362e-06   \n",
      "2999997           Cheque    22        15                 81  2.208965e-05   \n",
      "2999998  Cash Withdrawal    22        18                107  1.522045e-06   \n",
      "2999999     Cross-border     5         5                105  2.261997e-06   \n",
      "\n",
      "         transaction_count  rolling_24h_amount  Is_laundering  \n",
      "0                       84                0.00              0  \n",
      "1                      137                0.00              0  \n",
      "2                        2                0.00              0  \n",
      "3                       90                0.00              0  \n",
      "4                      126                0.00              0  \n",
      "...                    ...                 ...            ...  \n",
      "2999995                 63          1591216.30              1  \n",
      "2999996                107           362304.72              1  \n",
      "2999997                  4                0.00              1  \n",
      "2999998                105           179678.51              1  \n",
      "2999999                 98           218380.70              1  \n",
      "\n",
      "[3000000 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "data['Hour'] = pd.to_datetime(data['Time']).dt.hour\n",
    "\n",
    "data['Date_Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "data['Date_Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "data['Date_Day'] = pd.to_datetime(data['Date']).dt.day\n",
    "\n",
    "data.drop(columns=['Laundering_type'], inplace=True)\n",
    "data.drop(columns=['Time', 'Date'], inplace=True)\n",
    "# Zorg dat alle accounts strings zijn en geen lege waarden bevatten\n",
    "data[\"Sender_account\"] = data[\"Sender_account\"].astype(str)\n",
    "data[\"Receiver_account\"] = data[\"Receiver_account\"].astype(str)\n",
    "\n",
    "# remove empty columns\n",
    "data.dropna(subset=[\"Sender_account\", \"Receiver_account\"], inplace=True)\n",
    "\n",
    "data[\"Sender_account\"] = data[\"Sender_account\"].astype(str)\n",
    "data[\"Receiver_account\"] = data[\"Receiver_account\"].astype(str)\n",
    "data.dropna(subset=[\"Sender_account\", \"Receiver_account\"], inplace=True)\n",
    "\n",
    "#graph\n",
    "G = nx.DiGraph()\n",
    "edges = list(zip(data[\"Sender_account\"], data[\"Receiver_account\"], data[\"Amount\"]))\n",
    "G.add_weighted_edges_from(edges)\n",
    "G_ig = ig.Graph.TupleList(list(zip(data[\"Sender_account\"], data[\"Receiver_account\"])), directed=True)\n",
    "\n",
    "data[\"degree_centrality\"] = data[\"Sender_account\"].map(dict(zip(G_ig.vs[\"name\"], G_ig.degree()))).fillna(0)\n",
    "pagerank_scores = G_ig.pagerank()\n",
    "data[\"pagerank\"] = data[\"Sender_account\"].map(dict(zip(G_ig.vs[\"name\"], pagerank_scores))).fillna(0)\n",
    "\n",
    "window_size = 50\n",
    "data[\"rolling_24h_amount\"] = data.groupby(\"Sender_account\")[\"Amount\"].rolling(window_size).sum().reset_index(0, drop=True).fillna(0)\n",
    "data[\"transaction_count\"] = data.groupby(\"Sender_account\")[\"Amount\"].transform(\"count\")\n",
    "\n",
    "features = data.drop(columns=[\n",
    "    'Is_laundering',\n",
    "   'rolling_24h_amount',\n",
    "    'Sender_bank_location',\n",
    "    'Payment_currency',\n",
    "    'Received_currency'\n",
    "]).copy()\n",
    "features[\"degree_centrality\"] = data[\"degree_centrality\"]\n",
    "features[\"pagerank\"] = data[\"pagerank\"]\n",
    "features[\"rolling_24h_amount\"] = data[\"rolling_24h_amount\"]\n",
    "features[\"transaction_count\"] = data[\"transaction_count\"]\n",
    "features.drop(columns=['Date_Year', 'Date_Month'], inplace=True, errors='ignore')\n",
    "target = data['Is_laundering']\n",
    "\n",
    "data_standardized = pd.DataFrame(features)\n",
    "data_standardized['Is_laundering'] = target.reset_index(drop=True)\n",
    "print(data_standardized)\n",
    "\n",
    "# 2. Split in features en target\n",
    "X = data_standardized.drop(columns=[\"Is_laundering\"])\n",
    "y = data_standardized[\"Is_laundering\"]\n",
    "\n",
    "# 3. Train/test split met stratificatie (belangrijk bij imbalanced data)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "# Further splitting the train set into Train and Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18654b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical columns\n",
    "categorical_cols = ['Sender_account', 'Receiver_account', 'Payment_type','Receiver_bank_location']\n",
    "numerical_cols = ['Hour', 'Amount', 'degree_centrality', 'pagerank', 'rolling_24h_amount', 'transaction_count']\n",
    "# Combine train, val, and test to fit LabelEncoder\n",
    "combined_data = pd.concat([X_train, X_val, X_test])\n",
    "\n",
    "# Applying Label Encoding to Categorical Columns (Train, Val, Test)\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(combined_data[col])  # Fit on the combined set to capture all categories\n",
    "    label_encoders[col] = encoder  # Store the encoder for future use\n",
    "\n",
    "    # Transform the columns\n",
    "    X_train[col] = encoder.transform(X_train[col])\n",
    "    X_val[col] = encoder.transform(X_val[col])\n",
    "    X_test[col] = encoder.transform(X_test[col])\n",
    "\n",
    "# Applying Standard Scaling to Numerical Columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numerical_cols])  # Fit only on training data\n",
    "\n",
    "# Transform the columns\n",
    "X_train[numerical_cols] = scaler.transform(X_train[numerical_cols])\n",
    "X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbff23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maak_pu_setting_van_echte_labels(y_true, label_ratio, random_state=42):\n",
    "    \"\"\"\n",
    "    Maakt PU-labels op basis van bestaande echte labels in y_true.\n",
    "    Alleen een percentage van de positieven wordt als gelabeld behouden.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Neem alleen echte positieven\n",
    "    positieve_indexen = np.where(y_true == 1)[0]\n",
    "    n_gelabeld = int(label_ratio * len(positieve_indexen))\n",
    "    gelabelde_indexen = np.random.choice(positieve_indexen, size=n_gelabeld, replace=False)\n",
    "\n",
    "    y_pu = np.zeros_like(y_true)\n",
    "    y_pu[gelabelde_indexen] = 1\n",
    "\n",
    "    return y_pu, y_true, gelabelde_indexen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b72aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = 0.001  # 0.1% echte positieven\n",
    "label_ratio = 0.2  # 20% labeling probability (c)\n",
    "\n",
    "y_train_pu, y_train_true, gelabelde_indexen = maak_pu_setting_van_echte_labels(y_train, label_ratio=label_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb01bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bagging meta-estimator for PU learning.\n",
    "Any scikit-learn estimator should work as the base estimator.\n",
    "This implementation is fully compatible with scikit-learn, and is in fact based\n",
    "on the code of the sklearn.ensemble.BaggingClassifier class with very minor\n",
    "changes.\n",
    "\"\"\"\n",
    "\n",
    "# Author: Gilles Louppe <g.louppe@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "#\n",
    "#\n",
    "# Adapted for PU learning by Roy Wright <roy.w.wright@gmail.com>\n",
    "# (work in progress)\n",
    "#\n",
    "# A better idea: instead of a separate PU class, modify the original\n",
    "# sklearn BaggingClassifier so that the parameters max_samples\n",
    "# and bootstrap may be lists or dicts...\n",
    "# e.g. for a PU problem with 500 positives and 10000 unlabeled, we might set\n",
    "# max_samples = [500, 500]     (to balance P and U in each bag)\n",
    "# bootstrap = [True, False]    (to only bootstrap the unlabeled)\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\"BaggingPuClassifier\"]\n",
    "\n",
    "MAX_INT = np.iinfo(np.int32).max\n",
    "\n",
    "\n",
    "def _generate_indices(random_state, bootstrap, n_population, n_samples):\n",
    "    \"\"\"Draw randomly sampled indices.\"\"\"\n",
    "    # Draw sample indices\n",
    "    if bootstrap:\n",
    "        indices = random_state.randint(0, n_population, n_samples)\n",
    "    else:\n",
    "        indices = sample_without_replacement(n_population, n_samples,\n",
    "                                             random_state=random_state)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def _generate_bagging_indices(random_state, bootstrap_features,\n",
    "                              bootstrap_samples, n_features, n_samples,\n",
    "                              max_features, max_samples):\n",
    "    \"\"\"Randomly draw feature and sample indices.\"\"\"\n",
    "    # Get valid random state\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    # Draw indices\n",
    "    feature_indices = _generate_indices(random_state, bootstrap_features,\n",
    "                                        n_features, max_features)\n",
    "    sample_indices = _generate_indices(random_state, bootstrap_samples,\n",
    "                                       n_samples, max_samples)\n",
    "\n",
    "    return feature_indices, sample_indices\n",
    "\n",
    "\n",
    "def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,\n",
    "                               seeds, total_n_estimators, verbose):\n",
    "    \"\"\"Private function used to build a batch of estimators within a job.\"\"\"\n",
    "    # Retrieve settings\n",
    "    n_samples, n_features = X.shape\n",
    "    max_features = ensemble._max_features\n",
    "    max_samples = ensemble._max_samples\n",
    "    bootstrap = ensemble.bootstrap\n",
    "    bootstrap_features = ensemble.bootstrap_features\n",
    "    support_sample_weight = has_fit_parameter(ensemble.base_estimator_,\n",
    "                                              \"sample_weight\")\n",
    "    if not support_sample_weight and sample_weight is not None:\n",
    "        raise ValueError(\"The base estimator doesn't support sample weight\")\n",
    "\n",
    "    # Build estimators\n",
    "    estimators = []\n",
    "    estimators_features = []\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        if verbose > 1:\n",
    "            print(\"Building estimator %d of %d for this parallel run \"\n",
    "                  \"(total %d)...\" % (i + 1, n_estimators, total_n_estimators))\n",
    "\n",
    "        random_state = np.random.RandomState(seeds[i])\n",
    "        estimator = ensemble._make_estimator(append=False,\n",
    "                                             random_state=random_state)\n",
    "\n",
    "        # ============ MAIN MODIFICATION FOR PU LEARNING =============\n",
    "        iP = [pair[0] for pair in enumerate(y) if pair[1] == 1]\n",
    "        iU = [pair[0] for pair in enumerate(y) if pair[1] < 1]\n",
    "        features, indices = _generate_bagging_indices(random_state,\n",
    "                                                      bootstrap_features,\n",
    "                                                      bootstrap, n_features,\n",
    "                                                      len(iU), max_features,\n",
    "                                                      max_samples)\n",
    "        indices = [iU[i] for i in indices] + iP\n",
    "        # ============================================================\n",
    "\n",
    "        # Draw samples, using sample weights, and then fit\n",
    "        if support_sample_weight:\n",
    "            if sample_weight is None:\n",
    "                curr_sample_weight = np.ones((n_samples,))\n",
    "            else:\n",
    "                curr_sample_weight = sample_weight.copy()\n",
    "\n",
    "            if bootstrap:\n",
    "                sample_counts = np.bincount(indices, minlength=n_samples)\n",
    "                curr_sample_weight *= sample_counts\n",
    "            else:\n",
    "                not_indices_mask = ~indices_to_mask(indices, n_samples)\n",
    "                curr_sample_weight[not_indices_mask] = 0\n",
    "\n",
    "            estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)\n",
    "\n",
    "        # Draw samples, using a mask, and then fit\n",
    "        else:\n",
    "            estimator.fit((X[indices])[:, features], y[indices])\n",
    "\n",
    "        estimators.append(estimator)\n",
    "        estimators_features.append(features)\n",
    "\n",
    "    return estimators, estimators_features\n",
    "\n",
    "\n",
    "def _parallel_predict_proba(estimators, estimators_features, X, n_classes):\n",
    "    \"\"\"Private function used to compute (proba-)predictions within a job.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    proba = np.zeros((n_samples, n_classes))\n",
    "\n",
    "    for estimator, features in zip(estimators, estimators_features):\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            proba_estimator = estimator.predict_proba(X[:, features])\n",
    "\n",
    "            if n_classes == len(estimator.classes_):\n",
    "                proba += proba_estimator\n",
    "\n",
    "            else:  # pragma: no cover\n",
    "                proba[:, estimator.classes_] += \\\n",
    "                    proba_estimator[:, range(len(estimator.classes_))]\n",
    "\n",
    "        else:\n",
    "            # Resort to voting\n",
    "            predictions = estimator.predict(X[:, features])\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                proba[i, predictions[i]] += 1\n",
    "\n",
    "    return proba\n",
    "\n",
    "\n",
    "def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n",
    "    \"\"\"Private function used to compute log probabilities within a job.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    log_proba = np.empty((n_samples, n_classes))\n",
    "    log_proba.fill(-np.inf)\n",
    "    all_classes = np.arange(n_classes, dtype=np.int)\n",
    "\n",
    "    for estimator, features in zip(estimators, estimators_features):\n",
    "        log_proba_estimator = estimator.predict_log_proba(X[:, features])\n",
    "\n",
    "        if n_classes == len(estimator.classes_):\n",
    "            log_proba = np.logaddexp(log_proba, log_proba_estimator)\n",
    "\n",
    "        else:  # pragma: no cover\n",
    "            log_proba[:, estimator.classes_] = np.logaddexp(\n",
    "                log_proba[:, estimator.classes_],\n",
    "                log_proba_estimator[:, range(len(estimator.classes_))])\n",
    "\n",
    "            missing = np.setdiff1d(all_classes, estimator.classes_)\n",
    "            log_proba[:, missing] = np.logaddexp(log_proba[:, missing],\n",
    "                                                 -np.inf)\n",
    "\n",
    "    return log_proba\n",
    "\n",
    "\n",
    "def _parallel_decision_function(estimators, estimators_features, X):\n",
    "    \"\"\"Private function used to compute decisions within a job.\"\"\"\n",
    "    return sum(estimator.decision_function(X[:, features])\n",
    "               for estimator, features in zip(estimators,\n",
    "                                              estimators_features))\n",
    "\n",
    "\n",
    "class BaseBaggingPU(with_metaclass(ABCMeta, BaseEnsemble)):\n",
    "    \"\"\"Base class for Bagging PU meta-estimator.\n",
    "    Warning: This class should not be used directly. Use derived classes\n",
    "    instead.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=10,\n",
    "                 max_samples=1.0,\n",
    "                 max_features=1.0,\n",
    "                 bootstrap=True,\n",
    "                 bootstrap_features=False,\n",
    "                 oob_score=True,\n",
    "                 warm_start=False,\n",
    "                 n_jobs=1,\n",
    "                 random_state=None,\n",
    "                 verbose=0):\n",
    "        super(BaseBaggingPU, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators)\n",
    "\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.bootstrap_features = bootstrap_features\n",
    "        self.oob_score = oob_score\n",
    "        self.warm_start = warm_start\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Build a Bagging ensemble of estimators from the training\n",
    "           set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values (1 for positive, 0 for unlabeled).\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Note that this is supported only if the base estimator supports\n",
    "            sample weighting.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n",
    "\n",
    "    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):\n",
    "        \"\"\"Build a Bagging ensemble of estimators from the training\n",
    "           set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values (1 for positive, 0 for unlabeled).\n",
    "        max_samples : int or float, optional (default=None)\n",
    "            Argument to use instead of self.max_samples.\n",
    "        max_depth : int, optional (default=None)\n",
    "            Override value used when constructing base estimator. Only\n",
    "            supported if the base estimator has a max_depth parameter.\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Note that this is supported only if the base estimator supports\n",
    "            sample weighting.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        self.y = y\n",
    "\n",
    "        # Convert data\n",
    "        X, y = check_X_y(X, y, ['csr', 'csc'])\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            check_consistent_length(y, sample_weight)\n",
    "\n",
    "        # Remap output\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        self._n_samples = n_samples\n",
    "        y = self._validate_y(y)\n",
    "\n",
    "        # Check parameters\n",
    "        self._validate_estimator()\n",
    "\n",
    "        if max_depth is not None:  # pragma: no cover\n",
    "            self.base_estimator_.max_depth = max_depth\n",
    "\n",
    "        # Validate max_samples\n",
    "        if max_samples is None:  # pragma: no cover\n",
    "            max_samples = self.max_samples\n",
    "        elif not isinstance(max_samples, (numbers.Integral, np.integer)):\n",
    "            max_samples = int(max_samples * sum(y < 1))\n",
    "\n",
    "        if not (0 < max_samples <= sum(y < 1)):\n",
    "            raise ValueError(\n",
    "                \"max_samples must be positive\"\n",
    "                \" and no larger than the number of unlabeled points\")\n",
    "\n",
    "        # Store validated integer row sampling value\n",
    "        self._max_samples = max_samples\n",
    "\n",
    "        # Validate max_features\n",
    "        if isinstance(self.max_features, (numbers.Integral, np.integer)):\n",
    "            max_features = self.max_features\n",
    "        else:  # float\n",
    "            max_features = int(self.max_features * self.n_features_)\n",
    "\n",
    "        if not (0 < max_features <= self.n_features_):\n",
    "            raise ValueError(\"max_features must be in (0, n_features]\")\n",
    "\n",
    "        # Store validated integer feature sampling value\n",
    "        self._max_features = max_features\n",
    "\n",
    "        # Other checks\n",
    "        if not self.bootstrap and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimation only available\"\n",
    "                             \" if bootstrap=True\")\n",
    "\n",
    "        if self.warm_start and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimate only available\"\n",
    "                             \" if warm_start=False\")\n",
    "\n",
    "        if hasattr(self, \"oob_score_\") and self.warm_start:  # pragma: no cover\n",
    "            del self.oob_score_  # pragma: no covr\n",
    "\n",
    "        if not self.warm_start or not hasattr(self, 'estimators_'):\n",
    "            # Free allocated memory, if any\n",
    "            self.estimators_ = []\n",
    "            self.estimators_features_ = []\n",
    "\n",
    "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
    "\n",
    "        if n_more_estimators < 0:  # pragma: no cover\n",
    "            raise ValueError('n_estimators=%d must be larger or equal to '\n",
    "                             'len(estimators_)=%d when warm_start==True'\n",
    "                             % (self.n_estimators, len(self.estimators_)))\n",
    "\n",
    "        if n_more_estimators == 0:\n",
    "            warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
    "                 \"fit new trees.\")\n",
    "            return self\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,\n",
    "                                                             self.n_jobs)\n",
    "        total_n_estimators = sum(n_estimators)\n",
    "\n",
    "        # Advance random state to state after training\n",
    "        # the first n_estimators\n",
    "        if self.warm_start and len(self.estimators_) > 0:  # pragma: no cover\n",
    "            random_state.randint(MAX_INT, size=len(self.estimators_))\n",
    "\n",
    "        seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n",
    "        self._seeds = seeds\n",
    "\n",
    "        all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "            delayed(_parallel_build_estimators)(\n",
    "                n_estimators[i],\n",
    "                self,\n",
    "                X,\n",
    "                y,\n",
    "                sample_weight,\n",
    "                seeds[starts[i]:starts[i + 1]],\n",
    "                total_n_estimators,\n",
    "                verbose=self.verbose)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        self.estimators_ += list(itertools.chain.from_iterable(\n",
    "            t[0] for t in all_results))\n",
    "        self.estimators_features_ += list(itertools.chain.from_iterable(\n",
    "            t[1] for t in all_results))\n",
    "\n",
    "        if self.oob_score:\n",
    "            self._set_oob_score(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def _set_oob_score(self, X, y):\n",
    "        \"\"\"Calculate out of bag predictions and score.\"\"\"\n",
    "\n",
    "    def _validate_y(self, y):  # pragma: no cover\n",
    "        # Default implementation\n",
    "        return column_or_1d(y, warn=True)\n",
    "\n",
    "    def _get_estimators_indices(self):\n",
    "        # Get drawn indices along both sample and feature axes\n",
    "        for seed in self._seeds:\n",
    "            # Operations accessing random_state must be performed identically\n",
    "            # to those in _parallel_build_estimators()\n",
    "            random_state = np.random.RandomState(seed)\n",
    "\n",
    "            # ============ MAIN MODIFICATION FOR PU LEARNING =============\n",
    "            iP = [pair[0] for pair in enumerate(self.y) if pair[1] == 1]\n",
    "            iU = [pair[0] for pair in enumerate(self.y) if pair[1] < 1]\n",
    "\n",
    "            feature_indices, sample_indices = _generate_bagging_indices(\n",
    "                random_state, self.bootstrap_features, self.bootstrap,\n",
    "                self.n_features_, len(iU), self._max_features,\n",
    "                self._max_samples)\n",
    "\n",
    "            sample_indices = [iU[i] for i in sample_indices] + iP\n",
    "            # ============================================================\n",
    "\n",
    "            yield feature_indices, sample_indices\n",
    "\n",
    "    @property\n",
    "    def estimators_samples_(self):\n",
    "        \"\"\"The subset of drawn samples for each base estimator.\n",
    "        Returns a dynamically generated list of boolean masks identifying\n",
    "        the samples used for fitting each member of the ensemble, i.e.,\n",
    "        the in-bag samples.\n",
    "        Note: the list is re-created at each call to the property in order\n",
    "        to reduce the object memory footprint by not storing the sampling\n",
    "        data. Thus fetching the property may be slower than expected.\n",
    "        \"\"\"\n",
    "        sample_masks = []\n",
    "        for _, sample_indices in self._get_estimators_indices():\n",
    "            mask = indices_to_mask(sample_indices, self._n_samples)\n",
    "            sample_masks.append(mask)\n",
    "\n",
    "        return sample_masks\n",
    "\n",
    "\n",
    "class BaggingPuClassifier(BaseBaggingPU, ClassifierMixin):\n",
    "    \"\"\"A Bagging PU classifier.\n",
    "    Adapted from sklearn.ensemble.BaggingClassifier, based on\n",
    "    A bagging SVM to learn from positive and unlabeled examples (2013)\n",
    "    by Mordelet and Vert\n",
    "    http://dx.doi.org/10.1016/j.patrec.2013.06.010\n",
    "    http://members.cbio.mines-paristech.fr/~jvert/svn/bibli/local/Mordelet2013bagging.pdf\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object or None, optional (default=None)\n",
    "        The base estimator to fit on random subsets of the dataset.\n",
    "        If None, then the base estimator is a decision tree.\n",
    "    n_estimators : int, optional (default=10)\n",
    "        The number of base estimators in the ensemble.\n",
    "    max_samples : int or float, optional (default=1.0)\n",
    "        The number of unlabeled samples to draw to train each base estimator.\n",
    "    max_features : int or float, optional (default=1.0)\n",
    "        The number of features to draw from X to train each base estimator.\n",
    "        - If int, then draw max_features features.\n",
    "        - If float, then draw max_features * X.shape[1] features.\n",
    "    bootstrap : boolean, optional (default=True)\n",
    "        Whether samples are drawn with replacement.\n",
    "    bootstrap_features : boolean, optional (default=False)\n",
    "        Whether features are drawn with replacement.\n",
    "    oob_score : bool, optional (default=True)\n",
    "        Whether to use out-of-bag samples to estimate\n",
    "        the generalization error.\n",
    "    warm_start : bool, optional (default=False)\n",
    "        When set to True, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit\n",
    "        a whole new ensemble.\n",
    "    n_jobs : int, optional (default=1)\n",
    "        The number of jobs to run in parallel for both fit and predict.\n",
    "        If -1, then the number of jobs is set to the number of cores.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    verbose : int, optional (default=0)\n",
    "        Controls the verbosity of the building process.\n",
    "    Attributes\n",
    "    ----------\n",
    "    base_estimator_ : estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "    estimators_ : list of estimators\n",
    "        The collection of fitted base estimators.\n",
    "    estimators_samples_ : list of arrays\n",
    "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
    "        estimator. Each subset is defined by a boolean mask.\n",
    "    estimators_features_ : list of arrays\n",
    "        The subset of drawn features for each base estimator.\n",
    "    classes_ : array of shape = [n_classes]\n",
    "        The classes labels.\n",
    "    n_classes_ : int or list\n",
    "        The number of classes.\n",
    "    oob_score_ : float\n",
    "        Score of the training dataset obtained using an out-of-bag estimate.\n",
    "    oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
    "        Decision function computed with out-of-bag estimate on the training\n",
    "        set. Positive data points, and perhaps some of the unlabeled,\n",
    "        are left out during the bootstrap. In these cases,\n",
    "        oob_decision_function_ contains NaN.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=10,\n",
    "                 max_samples=1.0,\n",
    "                 max_features=1.0,\n",
    "                 bootstrap=True,\n",
    "                 bootstrap_features=False,\n",
    "                 oob_score=True,\n",
    "                 warm_start=False,\n",
    "                 n_jobs=1,\n",
    "                 random_state=None,\n",
    "                 verbose=0):\n",
    "\n",
    "        super(BaggingPuClassifier, self).__init__(\n",
    "            base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            max_samples=max_samples,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            bootstrap_features=bootstrap_features,\n",
    "            oob_score=oob_score,\n",
    "            warm_start=warm_start,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose)\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
    "        super(BaggingPuClassifier, self)._validate_estimator(\n",
    "            default=DecisionTreeClassifier())\n",
    "\n",
    "    def _set_oob_score(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "        n_classes_ = self.n_classes_\n",
    "\n",
    "        predictions = np.zeros((n_samples, n_classes_))\n",
    "\n",
    "        for estimator, samples, features in zip(self.estimators_,\n",
    "                                                self.estimators_samples_,\n",
    "                                                self.estimators_features_):\n",
    "            # Create mask for OOB samples\n",
    "            mask = ~samples\n",
    "\n",
    "            if hasattr(estimator, \"predict_proba\"):\n",
    "                predictions[mask, :] += estimator.predict_proba(\n",
    "                    (X[mask, :])[:, features])\n",
    "\n",
    "            else:\n",
    "                p = estimator.predict((X[mask, :])[:, features])\n",
    "                j = 0\n",
    "\n",
    "                for i in range(n_samples):\n",
    "                    if mask[i]:\n",
    "                        predictions[i, p[j]] += 1\n",
    "                        j += 1\n",
    "\n",
    "        # Modified: no warnings about non-OOB points (i.e. positives)\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            denominator = predictions.sum(axis=1)[:, np.newaxis]\n",
    "            oob_decision_function = predictions / denominator\n",
    "            oob_score = accuracy_score(y, np.argmax(predictions, axis=1))\n",
    "\n",
    "        self.oob_decision_function_ = oob_decision_function\n",
    "        self.oob_score_ = oob_score\n",
    "\n",
    "    def _validate_y(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\n",
    "        The predicted class of an input sample is computed as the class with\n",
    "        the highest mean predicted probability. If base estimators do not\n",
    "        implement a `predict_proba method, then it resorts to voting.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        predicted_probabilitiy = self.predict_proba(X)\n",
    "        return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),\n",
    "                                  axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the mean predicted class probabilities of the base estimators in the\n",
    "        ensemble. If base estimators do not implement a `predict_proba\n",
    "        method, then it resorts to voting and the predicted class probabilities\n",
    "        of an input sample represents the proportion of estimators predicting\n",
    "        each class.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute classes_.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "        # Check data\n",
    "        X = check_array(X, accept_sparse=['csr', 'csc'])\n",
    "\n",
    "        if self.n_features_ != X.shape[1]:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is {0} and \"\n",
    "                             \"input n_features is {1}.\"\n",
    "                             \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
    "                                                             self.n_jobs)\n",
    "\n",
    "        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "            delayed(_parallel_predict_proba)(\n",
    "                self.estimators_[starts[i]:starts[i + 1]],\n",
    "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                X,\n",
    "                self.n_classes_)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        proba = sum(all_proba) / self.n_estimators\n",
    "\n",
    "        return proba\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Predict class log-probabilities for X.\n",
    "        The predicted class log-probabilities of an input sample is computed as\n",
    "        the log of the mean predicted class probabilities of the base\n",
    "        estimators in the ensemble.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class log-probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute classes_.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "        if hasattr(self.base_estimator_, \"predict_log_proba\"):\n",
    "            # Check data\n",
    "            X = check_array(X, accept_sparse=['csr', 'csc'])\n",
    "\n",
    "            if self.n_features_ != X.shape[1]:\n",
    "                raise ValueError(\"Number of features of the model must \"\n",
    "                                 \"match the input. Model n_features is {0} \"\n",
    "                                 \"and input n_features is {1} \"\n",
    "                                 \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "            # Parallel loop\n",
    "            n_jobs, n_estimators, starts = _partition_estimators(\n",
    "                self.n_estimators, self.n_jobs)\n",
    "\n",
    "            all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "                delayed(_parallel_predict_log_proba)(\n",
    "                    self.estimators_[starts[i]:starts[i + 1]],\n",
    "                    self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                    X,\n",
    "                    self.n_classes_)\n",
    "                for i in range(n_jobs))\n",
    "\n",
    "            # Reduce\n",
    "            log_proba = all_log_proba[0]\n",
    "\n",
    "            for j in range(1, len(all_log_proba)):  # pragma: no cover\n",
    "                log_proba = np.logaddexp(log_proba, all_log_proba[j])\n",
    "\n",
    "            log_proba -= np.log(self.n_estimators)\n",
    "\n",
    "            return log_proba\n",
    "        # else, the base estimator has no predict_log_proba, so...\n",
    "        return np.log(self.predict_proba(X))\n",
    "\n",
    "    @if_delegate_has_method(delegate='base_estimator')\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Average of the decision functions of the base classifiers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The columns correspond\n",
    "            to the classes in sorted order, as they appear in the attribute\n",
    "            `classes_. Regression and binary classification are special\n",
    "            cases with `k == 1, otherwise k==n_classes.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        # Check data\n",
    "        X = check_array(X, accept_sparse=['csr', 'csc'])\n",
    "\n",
    "        if self.n_features_ != X.shape[1]:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is {0} and \"\n",
    "                             \"input n_features is {1} \"\n",
    "                             \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
    "                                                             self.n_jobs)\n",
    "\n",
    "        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "            delayed(_parallel_decision_function)(\n",
    "                self.estimators_[starts[i]:starts[i + 1]],\n",
    "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                X)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        decisions = sum(all_decisions) / self.n_estimators\n",
    "\n",
    "        return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a80422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingPuClassifier(base_estimator=DecisionTreeClassifier(max_depth=30),\n",
       "                    max_samples=1364917, n_estimators=175, n_jobs=-1,\n",
       "                    oob_score=False, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingPuClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingPuClassifier(base_estimator=DecisionTreeClassifier(max_depth=30),\n",
       "                    max_samples=1364917, n_estimators=175, n_jobs=-1,\n",
       "                    oob_score=False, random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=30)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=30)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingPuClassifier(base_estimator=DecisionTreeClassifier(max_depth=30),\n",
       "                    max_samples=1364917, n_estimators=175, n_jobs=-1,\n",
       "                    oob_score=False, random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Train de Bagging PU Classifier\n",
    "pu_clf = BaggingPuClassifier(\n",
    "   base_estimator=DecisionTreeClassifier(max_depth=30),\n",
    "    n_estimators=175,\n",
    "    max_samples=1364917,        \n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Trainen op de PU-versie van y_train\n",
    "pu_clf.fit(X_train.values, y_train_pu)\n",
    "\n",
    "# best params={'n_estimators': 175, 'max_depth': 30, 'max_samples': 1364917}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52a764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluatie (PUBagging): met threshold 0.05\n",
      "Accuracy: 0.9979\n",
      "Precision: 0.1797\n",
      "Recall: 0.2941\n",
      "F1-score: 0.2231\n",
      "ROC AUC: 0.7206\n",
      "PR AUC: 0.1275\n"
     ]
    }
   ],
   "source": [
    "y_proba = pu_clf.predict_proba(X_test)[:, 1]\n",
    "thresholds = np.linspace(0.0, 1.0, 101)\n",
    "f1_scores = [f1_score(y_test, (y_proba >= t).astype(int)) for t in thresholds]\n",
    "pr_auc_scores = [average_precision_score(y_test, (y_proba >= t).astype(int)) for t in thresholds]\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluatie\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "pr_auc = average_precision_score(y_test, y_proba)\n",
    "\n",
    "print(\"Model evaluatie (PUBagging): met threshold\", best_threshold)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c0a665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhwAAAHUCAYAAABxrfE4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABx/0lEQVR4nO3deVxU9foH8M+wDQPKCLI5hruiiLuJaIYb4oLozXIhSdLQ3IhAM+zmVgEqaYl7ueUSeVO8roRpYiQoEiQoLqmIXEFcEBQREM/vD3+cHEGEmpmjzOd9X+d1nXOe+Z5nRszH73ZkgiAIICIiItIiA6kTICIiotqPBQcRERFpHQsOIiIi0joWHERERKR1LDiIiIhI61hwEBERkdax4CAiIiKtY8FBREREWseCg4iIiLSOBQe9VE6dOoV3330XTZs2hampKerUqYPOnTtj0aJFuH37tlbvnZycDDc3NyiVSshkMnz11Vcav4dMJsO8efM03u7zbNy4ETKZDDKZDEeOHKlwXRAEtGjRAjKZDL179/5b91i5ciU2btxYo/ccOXLkmTkR0cvFSOoEiKrrm2++wZQpU+Do6IiZM2fCyckJpaWlOHnyJFavXo34+HhERUVp7f7jx49HYWEhIiMjYWlpiSZNmmj8HvHx8XjllVc03m511a1bF+vWratQVMTGxuLixYuoW7fu32575cqVsLa2hq+vb7Xf07lzZ8THx8PJyelv35eIXgwsOOilEB8fj8mTJ8Pd3R27du2CXC4Xr7m7uyMoKAjR0dFazSEtLQ1+fn4YNGiQ1u7RvXt3rbVdHaNGjcLWrVuxYsUKWFhYiOfXrVsHV1dXFBQU6CSP0tJSyGQyWFhYSP6dEJFmcEiFXgohISGQyWRYu3atWrFRzsTEBF5eXuLrR48eYdGiRWjdujXkcjlsbW3xzjvvICsrS+19vXv3hrOzMxITE9GrVy+YmZmhWbNmCAsLw6NHjwD8Ndzw8OFDrFq1Shx6AIB58+aJv35S+XsyMjLEc4cPH0bv3r1Rv359KBQKNGrUCCNGjMD9+/fFmMqGVNLS0jBs2DBYWlrC1NQUHTt2xKZNm9Riyocevv/+e3zyySdQqVSwsLBA//79ce7cuep9yQDGjBkDAPj+++/Fc/n5+dixYwfGjx9f6Xvmz58PFxcXWFlZwcLCAp07d8a6devw5HMhmzRpgtOnTyM2Nlb8/sp7iMpz37x5M4KCgtCwYUPI5XL8+eefFYZUbt68CQcHB/To0QOlpaVi+2fOnIG5uTl8fHyq/VmJSLdYcNALr6ysDIcPH0aXLl3g4OBQrfdMnjwZs2bNgru7O3bv3o3PPvsM0dHR6NGjB27evKkWm5OTg7fffhtjx47F7t27MWjQIAQHB2PLli0AgCFDhiA+Ph4A8OabbyI+Pl58XV0ZGRkYMmQITExMsH79ekRHRyMsLAzm5uYoKSl55vvOnTuHHj164PTp01i2bBl27twJJycn+Pr6YtGiRRXiZ8+ejStXruDbb7/F2rVrceHCBQwdOhRlZWXVytPCwgJvvvkm1q9fL577/vvvYWBggFGjRj3zs02aNAnbt2/Hzp078cYbb2D69On47LPPxJioqCg0a9YMnTp1Er+/p4e/goODkZmZidWrV2PPnj2wtbWtcC9ra2tERkYiMTERs2bNAgDcv38fb731Fho1aoTVq1dX63MSkQQEohdcTk6OAEAYPXp0teLT09MFAMKUKVPUzh8/flwAIMyePVs85+bmJgAQjh8/rhbr5OQkeHh4qJ0DIEydOlXt3Ny5c4XK/hht2LBBACBcvnxZEARB+PHHHwUAQkpKSpW5AxDmzp0rvh49erQgl8uFzMxMtbhBgwYJZmZmwp07dwRBEIRffvlFACAMHjxYLW779u0CACE+Pr7K+5bnm5iYKLaVlpYmCIIgvPrqq4Kvr68gCILQtm1bwc3N7ZntlJWVCaWlpcKCBQuE+vXrC48ePRKvPeu95fd7/fXXn3ntl19+UTu/cOFCAYAQFRUljBs3TlAoFMKpU6eq/IxEJC32cFCt88svvwBAhcmJ3bp1Q5s2bXDo0CG18/b29ujWrZvaufbt2+PKlSsay6ljx44wMTHBxIkTsWnTJly6dKla7zt8+DD69etXoWfH19cX9+/fr9DT8uSwEvD4cwCo0Wdxc3ND8+bNsX79eqSmpiIxMfGZwynlOfbv3x9KpRKGhoYwNjbGnDlzcOvWLeTm5lb7viNGjKh27MyZMzFkyBCMGTMGmzZtQkREBNq1a1ft9xOR7rHgoBeetbU1zMzMcPny5WrF37p1CwDQoEGDCtdUKpV4vVz9+vUrxMnlchQVFf2NbCvXvHlz/Pzzz7C1tcXUqVPRvHlzNG/eHF9//XWV77t169YzP0f59Sc9/VnK57vU5LPIZDK8++672LJlC1avXo1WrVqhV69elcaeOHECAwYMAPB4FdFvv/2GxMREfPLJJzW+b2Wfs6ocfX198eDBA9jb23PuBtFLgAUHvfAMDQ3Rr18/JCUlVZj0WZnyv3Szs7MrXLt27Rqsra01lpupqSkAoLi4WO380/NEAKBXr17Ys2cP8vPzkZCQAFdXVwQEBCAyMvKZ7devX/+ZnwOARj/Lk3x9fXHz5k2sXr0a77777jPjIiMjYWxsjL1792LkyJHo0aMHunbt+rfuWdnk22fJzs7G1KlT0bFjR9y6dQszZsz4W/ckIt1hwUEvheDgYAiCAD8/v0onWZaWlmLPnj0AgL59+wKAOOmzXGJiItLT09GvXz+N5VW+0uLUqVNq58tzqYyhoSFcXFywYsUKAMDvv//+zNh+/frh8OHDYoFR7rvvvoOZmZnWlow2bNgQM2fOxNChQzFu3LhnxslkMhgZGcHQ0FA8V1RUhM2bN1eI1VSvUVlZGcaMGQOZTIYDBw4gNDQUERER2Llz5z9um4i0h/tw0EvB1dUVq1atwpQpU9ClSxdMnjwZbdu2RWlpKZKTk7F27Vo4Oztj6NChcHR0xMSJExEREQEDAwMMGjQIGRkZ+PTTT+Hg4IAPP/xQY3kNHjwYVlZWmDBhAhYsWAAjIyNs3LgRV69eVYtbvXo1Dh8+jCFDhqBRo0Z48OCBuBKkf//+z2x/7ty52Lt3L/r06YM5c+bAysoKW7duxb59+7Bo0SIolUqNfZanhYWFPTdmyJAhWLJkCby9vTFx4kTcunUL4eHhlS5dbteuHSIjI/HDDz+gWbNmMDU1/VvzLubOnYtff/0VMTExsLe3R1BQEGJjYzFhwgR06tQJTZs2rXGbRKR9LDjopeHn54du3bph6dKlWLhwIXJycmBsbIxWrVrB29sb06ZNE2NXrVqF5s2bY926dVixYgWUSiUGDhyI0NDQSuds/F0WFhaIjo5GQEAAxo4di3r16uG9997DoEGD8N5774lxHTt2RExMDObOnYucnBzUqVMHzs7O2L17tzgHojKOjo44duwYZs+ejalTp6KoqAht2rTBhg0barRjp7b07dsX69evx8KFCzF06FA0bNgQfn5+sLW1xYQJE9Ri58+fj+zsbPj5+eHu3bto3Lix2j4l1XHw4EGEhobi008/Veup2rhxIzp16oRRo0YhLi4OJiYmmvh4RKRBMkF4YnceIiIiIi3gHA4iIiLSOhYcREREpHUsOIiIiEjrWHAQERGR1rHgICIiIq1jwUFERERax4KDiIiItK5Wbvyl6DTt+UFEL7m8xOVSp0CkdaZa/ltKk39fFCXzz2RVamXBQUREVC0ydvTrCr9pIiIi0jr2cBARkf6SyaTOQG+w4CAiIv3FIRWd4TdNREREWsceDiIi0l8cUtEZFhxERKS/OKSiM/ymiYiISOvYw0FERPqLQyo6w4KDiIj0F4dUdIbfNBEREWkdeziIiEh/cUhFZ1hwEBGR/uKQis7wmyYiIiKtYw8HERHpLw6p6AwLDiIi0l8cUtEZftNERESkdezhICIi/cUhFZ1hwUFERPqLQyo6w2+aiIiItI49HEREpL/Yw6EzLDiIiEh/GXAOh66wtCMiIiKtYw8HERHpLw6p6AwLDiIi0l9cFqszLO2IiIhI69jDQURE+otDKjrDgoOIiPQXh1R0hqUdERERaR17OIiISH9xSEVnWHAQEZH+4pCKzrC0IyIiIq1jDwcREekvDqnoDAsOIiLSXxxS0RmWdkRERKR17OEgIiL9xSEVnWHBQURE+otDKjrD0o6IiEjHHj58iH//+99o2rQpFAoFmjVrhgULFuDRo0dijCAImDdvHlQqFRQKBXr37o3Tp0+rtVNcXIzp06fD2toa5ubm8PLyQlZWllpMXl4efHx8oFQqoVQq4ePjgzt37qjFZGZmYujQoTA3N4e1tTX8/f1RUlKiFpOamgo3NzcoFAo0bNgQCxYsgCAI1f7MLDiIiEh/yQw0d9TAwoULsXr1aixfvhzp6elYtGgRFi9ejIiICDFm0aJFWLJkCZYvX47ExETY29vD3d0dd+/eFWMCAgIQFRWFyMhIxMXF4d69e/D09ERZWZkY4+3tjZSUFERHRyM6OhopKSnw8fERr5eVlWHIkCEoLCxEXFwcIiMjsWPHDgQFBYkxBQUFcHd3h0qlQmJiIiIiIhAeHo4lS5ZU/6sWalKevCQUnaZJnQKR1uUlLpc6BSKtM9XywL9i6EqNtVW0Z0q1Yz09PWFnZ4d169aJ50aMGAEzMzNs3rwZgiBApVIhICAAs2bNAvC4N8POzg4LFy7EpEmTkJ+fDxsbG2zevBmjRo0CAFy7dg0ODg7Yv38/PDw8kJ6eDicnJyQkJMDFxQUAkJCQAFdXV5w9exaOjo44cOAAPD09cfXqVahUKgBAZGQkfH19kZubCwsLC6xatQrBwcG4fv065HI5ACAsLAwRERHIysqCrBpDU+zhICIi0oDi4mIUFBSoHcXFxZXGvvbaazh06BDOnz8PAPjjjz8QFxeHwYMHAwAuX76MnJwcDBgwQHyPXC6Hm5sbjh07BgBISkpCaWmpWoxKpYKzs7MYEx8fD6VSKRYbANC9e3colUq1GGdnZ7HYAAAPDw8UFxcjKSlJjHFzcxOLjfKYa9euISMjo1rfDwsOIiLSXzKZxo7Q0FBxnkT5ERoaWultZ82ahTFjxqB169YwNjZGp06dEBAQgDFjxgAAcnJyAAB2dnZq77OzsxOv5eTkwMTEBJaWllXG2NraVri/ra2tWszT97G0tISJiUmVMeWvy2Oeh6tUiIhIf2lwWWxwcDACAwPVzj3ZI/CkH374AVu2bMG2bdvQtm1bpKSkICAgACqVCuPGjfsrvaeGKgRBeO7wxdMxlcVrIqZ8RkZ1hlMAFhxEREQaIZfLn1lgPG3mzJn4+OOPMXr0aABAu3btcOXKFYSGhmLcuHGwt7cH8Lj3oEGDBuL7cnNzxZ4Fe3t7lJSUIC8vT62XIzc3Fz169BBjrl+/XuH+N27cUGvn+PHjatfz8vJQWlqqFvN0T0Zubi6Air0wz8IhFSIi0l8aHFKpifv378PAQP2vYENDQ3FZbNOmTWFvb4+DBw+K10tKShAbGysWE126dIGxsbFaTHZ2NtLS0sQYV1dX5Ofn48SJE2LM8ePHkZ+frxaTlpaG7OxsMSYmJgZyuRxdunQRY44ePaq2VDYmJgYqlQpNmjSp1mdmwUFERPpLomWxQ4cOxRdffIF9+/YhIyMDUVFRWLJkCf71r389TksmQ0BAAEJCQhAVFYW0tDT4+vrCzMwM3t7eAAClUokJEyYgKCgIhw4dQnJyMsaOHYt27dqhf//+AIA2bdpg4MCB8PPzQ0JCAhISEuDn5wdPT084OjoCAAYMGAAnJyf4+PggOTkZhw4dwowZM+Dn5wcLCwsAj5fWyuVy+Pr6Ii0tDVFRUQgJCUFgYODLO6RSUFCAw4cPw9HREW3atJE6HSIiIo2LiIjAp59+iilTpiA3NxcqlQqTJk3CnDlzxJiPPvoIRUVFmDJlCvLy8uDi4oKYmBjUrVtXjFm6dCmMjIwwcuRIFBUVoV+/fti4cSMMDQ3FmK1bt8Lf319czeLl5YXly/9aVm9oaIh9+/ZhypQp6NmzJxQKBby9vREeHi7GKJVKHDx4EFOnTkXXrl1haWmJwMDACnNWqiL5PhwjR47E66+/jmnTpqGoqAgdOnRARkYGBEFAZGQkRowYUeM2uQ8H6QPuw0H6QOv7cLyx7vlB1VS0c4LG2qqNJB9SOXr0KHr16gUAiIqKgiAIuHPnDpYtW4bPP/9c4uyIiKg2k8lkGjuoapIXHPn5+bCysgIAREdHizutDRkyBBcuXJA4OyIiItIEyQsOBwcHxMfHo7CwENHR0eIYU15eHkxNTSXOjoiIajP2cOiO5JNGAwIC8Pbbb6NOnTpo3LgxevfuDeDxUEu7du2kTY6IiGo31gk6I3nBMWXKFHTr1g1Xr16Fu7u7uC65WbNmnMNBRERUS0hecABA165d0bVrV7VzQ4YMkSgbIiLSFxwK0R3JC47x48dXeX39+vU6yoSIiPQNCw7dkbzgyMvLU3tdWlqKtLQ03LlzB3379pUoKyIiItIkyQuOqKioCucePXqEKVOmoFmzZhJkRERE+oI9HLoj+bLYyhgYGODDDz/E0qVLpU6FiIhqMS6L1Z0XsuAAgIsXL+Lhw4dSp0FEREQaIPmQytMPfhEEAdnZ2di3bx/GjRsnUVZERKQX2DGhM5IXHMnJyWqvDQwMYGNjgy+//PK5K1iIiIj+CQ6F6I7kBccvv/widQpERESkZZIXHOVu3LiBc+fOQSaToVWrVrCxsZE6JSIiquXYw6E7kk8aLSwsxPjx49GgQQO8/vrr6NWrF1QqFSZMmID79+9LnR4REdViXKWiO5IXHIGBgYiNjcWePXtw584d3LlzB//9738RGxuLoKAgqdMjIiIiDZB8SGXHjh348ccfxafEAsDgwYOhUCgwcuRIrFq1SrrkiIioVmPPhO5IXnDcv38fdnZ2Fc7b2tpySIWIiLSL9YbOSD6k4urqirlz5+LBgwfiuaKiIsyfPx+urq4SZkZERESaInkPx9dff42BAwfilVdeQYcOHSCTyZCSkgJTU1P89NNPUqdHRES1GIdUdEfygsPZ2RkXLlzAli1bcPbsWQiCgNGjR+Ptt9+GQqGQOj0iIqrFWHDojuQFBwAoFAr4+flJnQYRERFpiSQFx+7duzFo0CAYGxtj9+7dVcZ6eXnpKCsiItI37OHQHUkKjuHDhyMnJwe2trYYPnz4M+NkMhnKysp0lxgREekX1hs6I0nB8ejRo0p/TURERLXTCzGHg4iISAocUtEdyffh8Pf3x7JlyyqcX758OQICAnSfEBER6Q0+S0V3JC84duzYgZ49e1Y436NHD/z4448SZERERESaJvmQyq1bt6BUKiuct7CwwM2bNyXIiIiI9AV7JnRH8h6OFi1aIDo6usL5AwcOoFmzZhJkRERE+oJDKrojeQ9HYGAgpk2bhhs3bqBv374AgEOHDuHLL7/EV199JW1yREREpBGS93CMHz8eX375JdatW4c+ffqgT58+2LJlC1atWsXdR4mISLtkGjxqoEmTJpX2kkydOhUAIAgC5s2bB5VKBYVCgd69e+P06dNqbRQXF2P69OmwtraGubk5vLy8kJWVpRaTl5cHHx8fKJVKKJVK+Pj44M6dO2oxmZmZGDp0KMzNzWFtbQ1/f3+UlJSoxaSmpsLNzQ0KhQINGzbEggULIAhCjT6z5AUHAEyePBlZWVm4fv06CgoKcOnSJbzzzjtSp0VERLWcVEMqiYmJyM7OFo+DBw8CAN566y0AwKJFi7BkyRIsX74ciYmJsLe3h7u7O+7evSu2ERAQgKioKERGRiIuLg737t2Dp6en2oaZ3t7eSElJQXR0NKKjo5GSkgIfHx/xellZGYYMGYLCwkLExcUhMjISO3bsQFBQkBhTUFAAd3d3qFQqJCYmIiIiAuHh4ViyZEnNvmuhpiXKS0DRaZrUKRBpXV7icqlTINI6Uy0P/DecHKWxtv636l9/+70BAQHYu3cvLly4AABQqVQICAjArFmzADzuzbCzs8PChQsxadIk5Ofnw8bGBps3b8aoUaMAANeuXYODgwP2798PDw8PpKenw8nJCQkJCXBxcQEAJCQkwNXVFWfPnoWjoyMOHDgAT09PXL16FSqVCgAQGRkJX19f5ObmwsLCAqtWrUJwcDCuX78OuVwOAAgLC0NERASysrKqXWxJ0sPRuXNn5OXlAQA6deqEzp07P/MgIiLSFk32cBQXF6OgoEDtKC4ufm4OJSUl2LJlC8aPHw+ZTIbLly8jJycHAwYMEGPkcjnc3Nxw7NgxAEBSUhJKS0vVYlQqFZydncWY+Ph4KJVKsdgAgO7du0OpVKrFODs7i8UGAHh4eKC4uBhJSUlijJubm1hslMdcu3YNGRkZ1f6uJZk0OmzYMDHxqp6lQkREpE2aXF0SGhqK+fPnq52bO3cu5s2bV+X7du3ahTt37sDX1xcAkJOTAwCws7NTi7Ozs8OVK1fEGBMTE1haWlaIKX9/+TPLnmZra6sW8/R9LC0tYWJiohbTpEmTCvcpv9a0adMqP185SQqOuXPnVvprIiKil1VwcDACAwPVzj3ZK/As69atw6BBg9R6GYCKxZAgCM8tkJ6OqSxeEzHlszFqUrBJviy23MmTJ5Geng6ZTIY2bdqgS5cuUqdERES1nQa3z5DL5dUqMJ505coV/Pzzz9i5c6d4zt7eHsDj3oMGDRqI53Nzc8WeBXt7e5SUlCAvL0+tlyM3Nxc9evQQY65fv17hnjdu3FBr5/jx42rX8/LyUFpaqhZT3tvx5H2Air0wVZF8lUpWVhZ69eqFbt264YMPPoC/vz9effVVvPbaa7h69arU6RERUS0m9cZfGzZsgK2tLYYMGSKea9q0Kezt7cWVK8DjeR6xsbFiMdGlSxcYGxurxWRnZyMtLU2McXV1RX5+Pk6cOCHGHD9+HPn5+WoxaWlpyM7OFmNiYmIgl8vFf/i7urri6NGjaktlY2JioFKpKgy1VEXygmP8+PEoLS1Feno6bt++jdu3byM9PR2CIGDChAlSp0dERKQVjx49woYNGzBu3DgYGf014CCTyRAQEICQkBBERUUhLS0Nvr6+MDMzg7e3NwBAqVRiwoQJCAoKwqFDh5CcnIyxY8eiXbt26N+/PwCgTZs2GDhwIPz8/JCQkICEhAT4+fnB09MTjo6OAIABAwbAyckJPj4+SE5OxqFDhzBjxgz4+fnBwsICwOOltXK5HL6+vkhLS0NUVBRCQkIQGBj4cg2p/Prrrzh27Jj44QHA0dERERERlT7UjYiISFOk3JL8559/RmZmJsaPH1/h2kcffYSioiJMmTIFeXl5cHFxQUxMDOrWrSvGLF26FEZGRhg5ciSKiorQr18/bNy4EYaGhmLM1q1b4e/vL65m8fLywvLlfy2pNzQ0xL59+zBlyhT07NkTCoUC3t7eCA8PF2OUSiUOHjyIqVOnomvXrrC0tERgYGCF+SrPI/k+HI6Ojti8eTO6deumdv7EiRPw9vbGn3/+WeM2uQ+HOkNDA/x70mCMHtwVdvUtkHOzAJv3JCDsm5/EiT+2VnXx+QfD0N+1DZR1FIj7/U8ELvoPLmbeAAA0amCFc/sXVNr+2zPXYefPyQCAFo1sEfLhcLh2aAYTY0Oc/vMa5q3Yi6MnL4jx4TNHwLVjc7Rt0QBnL19H99FhFdps20KFpR+/ha5tGyOv4D6+3RGH0LUVn7mjz7gPR80lnUzExvXrkH4mDTdu3MDSZSvQt9/jfw2WlpZi+bKvEPfrUWRlXUXdOnXg4toDH3wYBFvbv8apJ/j64GTiCbV2PQYNxqLwpeLrQe59ce3a/9Ri3p3gh4DAGVr8dLWTtvfhaPLBXo21lfG1p8baqo0k7+FYtGgRpk+fjhUrVqBLly6QyWQ4efIkPvjgA7UKi/6+IF93vPfma/CbsxlnLmajS9tGWDNvLAruPsCK748AALYvnYjSh2V4K2ANCgofwH9sX+xfPR2d3vgc9x+UIOt6Hpr0D1Zrd/yInggc546ffvtru92oiPdx4UouBk1ahqLiUkzz7oOdy95H26HzcP3W4x3yZDIZvvtvAl5t1xjOLRtWyLeuuSn2rpqGoyfP47Wxi9GysS3Wzh+L+0Ul+HrzYe19UVTrFRXdh6OjI4b96w0EBUxXu/bgwQOcTT+Die9PhqNjaxQUFGBRWAg+mDYZ32/fqRY74s2RmDLNX3wtNzWtcK8p0/wx4s2R4mszMzMNfxqil4vkBYevry/u378PFxcXcQzr4cOHMDIywvjx49W6mm7fvi1Vmi81l/ZNsTf2FKLjHhcGmdm3MXJgV3R2agTgca+ES/um6Dzic6RfejwT+YPQH5B5KAwjB3XBxqh4PHokiAVDOa8+HfBjTBIKix5PJKpfzxwtGtni/XlbkXbhGgDg02X/xfujXkeb5g3E9wct+hEAYG05uNKCY/TgrjCVG8FvzhaUlD7EmYvZaNnYFv5j+7LgoH/ktV5ueK2XW6XX6tatizXfblA79/Hsf+Pt0W8h+9o1NHhiyaKpqSmsbWyqvJe5uflzY0h6fMqr7khecPCJsNoXn3IR7735Glo0ssWfmblo16ohXDs2w0fhOwAAcpPHPwYPSh6K73n0SEBJ6UP06NgcG6PiK7TZqY0DOrZ2wIdh28Vzt+4UIv1SNrw9uyE5/SqKSx/ivRGvIedmAZLPVH/FkUv7pvg16U+UlP6Vz8Fj6fjMfxgaq+rjyrVbNf4OiP6Oe/fuQSaToe7/T54rt3/fHuzbuxtW9a3xWq/X8f6UqTA3r6MWs2Hdt1i7etXjZ2B4DITvuxNgbGKiy/SpOlhv6IzkBce4ceP+0fuLi4srbB0rPCqDzMDwGe/QP+EbDsKijgJ/RP0bZWUCDA1lmLtiL7ZHP9629lxGDq5cu4XPpnth2uffo7CoBB/49EUDGyXsrZWVtjluuCvSL2Uj4Y/Lauc931+O7V9Nwo3fwvHokYDc23cxbOoK5N8rqna+dvUtcOWaem9W7u3HvSP21hYsOEgniouL8fXScAwa4ok6df4qJgYPGYqGr7yC+tbW+PPCBSz76kucP3dWrXfEe+w7aOPkBAsLC6SlpmLZV1/if//LwrwFX0jxUYheCJIXHL///juMjY3Rrl07AMB///tfbNiwAU5OTpg3bx5MnvMvgsq2kjW0exXGDbo94x365y2PLhgz+FX4zt6EMxez0d6xIRbPeBPZN/Kxdc9xPHz4CGNmfItVc99G9tHFePiwDIePnxOHYJ5mKjfGqEFdEfZNxUmcX80ehRu376L/+K9QVFwC33/1wM5l7+O1sYuRc7Og2jk/PZdZ9ozzRNpQWlqKWTM+xKNHAj75dJ7atRFv/TUvo2XLVmjcuDHGjByB9DOn0capLQDAZ5yvGNPKsTUsLCwQ9KE/AgJnoF499a2oSVocUtEdyffhmDRpEs6fPw8AuHTpEkaNGgUzMzP85z//wUcfffTc9wcHByM/P1/tMLLjLqVPCgkYjvANB/Gfn5Jw+s9r+H5fIiK2HsbMd93FmOT0q+g+Ogx2vWag6YBPMGzaStRXmiPjfxV7E/7VvyPMTE2wda/6TP3e3VphcC9nvPPxBsT/cQkpZ7MQELodRcWlGDvUpUI7z3L9VgHsrNW7sG2s6v7/tbuVvYVIY0pLSzEzKAD/y8rCmm/Xq/VuVKaNU1sYGRmLz7ioTLsOHQEAmZmZmkyVNEDqjb/0ieQFx/nz59GxY0cAwH/+8x+4ublh27Zt2LhxI3bs2PHc98vlclhYWKgdHE5RpzA1wSPhkdq5skcCDAwq/vYX3HuAm3n30LyRDTo7NcLeI6cqxPgO74F9sam4mXdP7byZ6ePeqEeP1O/16NHz9/9/0vFTl/Fa5xYwNvrr97G/a2tcy73D4RTSqvJiI/PKFaxZt7FavRF//nkBDx+WwqaKCaJn088AAGysOYmU9JfkQyqCIIh/Qf3888/w9Hy8jtnBwQE3b96UMrVaY//RVMya4IGr2Xk4czEbHVu/Av+xffDdrgQx5o3+nXAj7x6u5tyGc0sVwme+iT1HTuFQwlm1tpo5WOO1zs0xfPqqCvc5fury4z0zPnsHIWsPoOhBKca/0QNNGtZXG55p5mCNOgo57KwtoJAbo32rxytV0i/loPRhGX44cBKzJw7GNwt8sGjdT2jRyAYzx3sg9JsDWvqGSF/cLyxU62X4X1YWzqanQ6lUwsbWFjM+9Ed6+hlErFiDR2VluHnj8T40SqUSxiYmuJqZiX17d6PX626oZ2mJSxcv4svFYWjdxgkdO3UGAPyRkoxTf/yBV7u5oE7dOjidlorFC0PRu09ftZUu9GJgx4TuSL7xV9++feHg4ID+/ftjwoQJOHPmDFq0aIHY2FiMGzcOGRkZNW6TG3+pq2Mmx9wpnvDq2wE2lnWQfSMf26OTELL2AEoflgEApoxxw4fv9Idt/brIuVmArXuPI3RttHi93PxpQ+E9pBtaDZ5T6XyKzk6NMG/qUHR2agRjIwOkX8pByNoDiPntjBjz0zcf4PWuLSu813HwHGRmP54s2raFCl8Fj/xr468f4xCylgXHk7jxV80lnjiO9959p8J5r2H/wvtTp2HwgH6Vvu/bDd/h1W4uyMnOxuyPZ+LPCxdw/34h7O0boJebG96fPA3KevUAAOlnTuOLz+Yj4/IllJSUoIFKhYGDhsB3/HtQKBTa/Hi1krY3/mo5U3MbCl5YPFBjbdVGkhccp06dwttvv43MzEwEBgaKj6ufPn06bt26hW3bttW4TRYcpA9YcJA+YMFRe0g+pNK+fXukpqZWOL948WK1/eCJiIg0jUMquiN5wfEsppVsFUxERKRJXF2iO5IXHAYGBlX+hpeVlT3zGhEREb0cJC84oqKi1F6XlpYiOTkZmzZtqrChFxERkSaxg0N3JC84hg0bVuHcm2++ibZt2+KHH37AhAkTJMiKiIj0gYEBKw5dkXzjr2dxcXHBzz//LHUaREREpAGS93BUpqioCBEREXjllVekToWIiGoxDqnojuQFh6WlpdqkUUEQcPfuXZiZmWHLli0SZkZERESaInnB8dVXX6m9NjAwgI2NDVxcXGBpyacqEhGR9nBZrO5IXnCMGzdO6hSIiEhPsd7QHckmjd6+fRtZWVlq506fPo13330XI0eO/FtbmhMREdGLSbKCY+rUqViyZIn4Ojc3F7169UJiYiKKi4vh6+uLzZs3S5UeERHpAZlMprGDqiZZwZGQkAAvLy/x9XfffQcrKyukpKTgv//9L0JCQrBixQqp0iMiIj3AgkN3JCs4cnJy0LRpU/H14cOH8a9//QtGRo+nlXh5eeHChQtSpUdEREQaJFnBYWFhgTt37oivT5w4ge7du4uvZTIZiouLJciMiIj0hUymuYOqJlnB0a1bNyxbtgyPHj3Cjz/+iLt376Jv377i9fPnz8PBwUGq9IiISA9wSEV3JFsW+9lnn6F///7YsmULHj58iNmzZ6vtuxEZGQk3Nzep0iMiIiINkqzg6NixI9LT03Hs2DHY29vDxcVF7fro0aPh5OQkUXZERKQP2DGhO5Ju/GVjY1Pp02IBYMiQITrOhoiI9A2HQnTnhX1aLBEREdUekm9tTkREJBV2cOgOCw4iItJbHFLRHQ6pEBERSeB///sfxo4di/r168PMzAwdO3ZEUlKSeF0QBMybNw8qlQoKhQK9e/fG6dOn1dooLi7G9OnTYW1tDXNzc3h5eVV4TlleXh58fHygVCqhVCrh4+Ojtg8WAGRmZmLo0KEwNzeHtbU1/P39UVJSohaTmpoKNzc3KBQKNGzYEAsWLIAgCNX+vJIXHIaGhsjNza1w/tatWzA0NJQgIyIi0hdSbfyVl5eHnj17wtjYGAcOHMCZM2fw5Zdfol69emLMokWLsGTJEixfvhyJiYmwt7eHu7s77t69K8YEBAQgKioKkZGRiIuLw7179+Dp6YmysjIxxtvbGykpKYiOjkZ0dDRSUlLg4+MjXi8rK8OQIUNQWFiIuLg4REZGYseOHQgKChJjCgoK4O7uDpVKhcTERERERCA8PFztmWjP/a6FmpQnWmBgYICcnBzY2tqqnb927RqaN2+OoqKiGrep6DRNU+kRvbDyEpdLnQKR1plqeeDfJTRWY20dDexeYYdsuVwOuVxeIfbjjz/Gb7/9hl9//bXStgRBgEqlQkBAAGbNmgXgcW+GnZ0dFi5ciEmTJiE/Px82NjbYvHkzRo0aBeDx350ODg7Yv38/PDw8kJ6eDicnJyQkJIjbTyQkJMDV1RVnz56Fo6MjDhw4AE9PT1y9ehUqlQrA472wfH19kZubCwsLC6xatQrBwcG4fv26+HnCwsIQERGBrKysag1NSdbDsWzZMixbtgwymQzffvut+HrZsmVYunQppk6ditatW0uVHhERUY2EhoaKwxblR2hoaKWxu3fvRteuXfHWW2/B1tYWnTp1wjfffCNev3z5MnJycjBgwADxnFwuh5ubG44dOwYASEpKQmlpqVqMSqWCs7OzGBMfHw+lUqm211X37t2hVCrVYpydncViAwA8PDxQXFwsDvHEx8fDzc1NrXjy8PDAtWvXkJGRUa3vR7JJo0uXLgXwuIpbvXq12vCJiYkJmjRpgtWrV0uVHhER6QFNzhkNDg5GYGCg2rnKejcA4NKlS1i1ahUCAwMxe/ZsnDhxAv7+/pDL5XjnnXeQk5MDALCzs1N7n52dHa5cuQLg8UNQTUxM1HbpLo8pf39lIwgAYGtrqxbz9H0sLS1hYmKiFtOkSZMK9ym/9uTDWJ9FsoLj8uXLAIA+ffpg586dFb4wIiIibdPkKpVnDZ9U5tGjR+jatStCQkIAAJ06dcLp06exatUqvPPOO8/MTxCE5+b8dExl8ZqIKZ+RUd3vUPJJo7/88otYbAiCUKMZr0RERC+jBg0aVHh8R5s2bZCZmQkAsLe3BwCxh6Fcbm6u2LNgb2+PkpIS5OXlVRlz/fr1Cve/ceOGWszT98nLy0NpaWmVMeULPp7uHXkWyQsOAPjuu+/Qrl07KBQKKBQKtG/fHps3b5Y6LSIiquWkWqXSs2dPnDt3Tu3c+fPn0bhxYwBA06ZNYW9vj4MHD4rXS0pKEBsbix49egAAunTpAmNjY7WY7OxspKWliTGurq7Iz8/HiRMnxJjjx48jPz9fLSYtLQ3Z2dliTExMDORyObp06SLGHD16VG2pbExMDFQqVYWhlmeRvOBYsmQJJk+ejMGDB2P79u344YcfMHDgQLz//vviPA8iIiJtkOrx9B9++CESEhIQEhKCP//8E9u2bcPatWsxdepUMa+AgACEhIQgKioKaWlp8PX1hZmZGby9vQEASqUSEyZMQFBQEA4dOoTk5GSMHTsW7dq1Q//+/QE87jUZOHAg/Pz8kJCQgISEBPj5+cHT0xOOjo4AgAEDBsDJyQk+Pj5ITk7GoUOHMGPGDPj5+cHCwgLA46W1crkcvr6+SEtLQ1RUFEJCQhAYGFjtzy75stimTZti/vz5amNWALBp0ybMmzdPnOtRE1wWS/qAy2JJH2h7WWzPxZUvS/07fpvZq0bxe/fuRXBwMC5cuICmTZsiMDAQfn5+4nVBEDB//nysWbMGeXl5cHFxwYoVK+Ds7CzGPHjwADNnzsS2bdtQVFSEfv36YeXKlXBwcBBjbt++DX9/f+zevRsA4OXlheXLl6vt+ZGZmYkpU6bg8OHDUCgU8Pb2Rnh4uNqclNTUVEydOhUnTpyApaUl3n//fcyZM+flKThMTU2RlpaGFi1aqJ2/cOEC2rVrhwcPHtS4TRYcpA9YcJA+0HbB8Vq45gqOuBk1Kzj0jeRDKi1atMD27dsrnP/hhx/QsmVLCTIiIiJ9IdWQij6S/OFt8+fPx6hRo3D06FH07NkTMpkMcXFxOHToUKWFCBEREb18JC84RowYgePHj2Pp0qXYtWsXBEGAk5MTTpw4gU6dOkmdHhER1WLsmdAdyQsO4PHSni1btkidBhER6RnWG7oj+RwOIiIiqv0k6+EwMDB4bleWTCbDw4cPdZQRERHpGw6p6I5kBUdUVNQzrx07dgwRERHc5pyIiLSK9YbuSFZwDBs2rMK5s2fPIjg4GHv27MHbb7+Nzz77TILMiIiISNNeiDkc165dg5+fH9q3b4+HDx8iJSUFmzZtQqNGjaROjYiIajHuw6E7khYc+fn5mDVrFlq0aIHTp0/j0KFD2LNnj9q2rURERNoi1cPb9JFkQyqLFi3CwoULYW9vj++//77SIRYiIiKqHSQrOD7++GMoFAq0aNECmzZtwqZNmyqN27lzp44zIyIifWHArgmdkazgeOeddzjmRUREkuJfQ7ojWcGxceNGqW5NREREOvZCbG1OREQkBfa06w4LDiIi0lsGrDd05oXYh4OIiIhqN/ZwEBGR3uKQiu6w4CAiIr3FekN3OKRCREREWsceDiIi0lsysItDV1hwEBGR3uIqFd3hkAoRERFpHXs4iIhIb3GViu6w4CAiIr3FekN3OKRCREREWsceDiIi0lt8PL3usOAgIiK9xXpDdzikQkRERFrHHg4iItJbXKWiOyw4iIhIb7He0B0OqRAREenYvHnzIJPJ1A57e3vxuiAImDdvHlQqFRQKBXr37o3Tp0+rtVFcXIzp06fD2toa5ubm8PLyQlZWllpMXl4efHx8oFQqoVQq4ePjgzt37qjFZGZmYujQoTA3N4e1tTX8/f1RUlKiFpOamgo3NzcoFAo0bNgQCxYsgCAINfrMLDiIiEhvGchkGjtqqm3btsjOzhaP1NRU8dqiRYuwZMkSLF++HImJibC3t4e7uzvu3r0rxgQEBCAqKgqRkZGIi4vDvXv34OnpibKyMjHG29sbKSkpiI6ORnR0NFJSUuDj4yNeLysrw5AhQ1BYWIi4uDhERkZix44dCAoKEmMKCgrg7u4OlUqFxMREREREIDw8HEuWLKnR5+WQChER6S0pR1SMjIzUejXKCYKAr776Cp988gneeOMNAMCmTZtgZ2eHbdu2YdKkScjPz8e6deuwefNm9O/fHwCwZcsWODg44Oeff4aHhwfS09MRHR2NhIQEuLi4AAC++eYbuLq64ty5c3B0dERMTAzOnDmDq1evQqVSAQC+/PJL+Pr64osvvoCFhQW2bt2KBw8eYOPGjZDL5XB2dsb58+exZMkSBAYGVnseDHs4iIiINKC4uBgFBQVqR3Fx8TPjL1y4AJVKhaZNm2L06NG4dOkSAODy5cvIycnBgAEDxFi5XA43NzccO3YMAJCUlITS0lK1GJVKBWdnZzEmPj4eSqVSLDYAoHv37lAqlWoxzs7OYrEBAB4eHiguLkZSUpIY4+bmBrlcrhZz7do1ZGRkVPv7YcFBRER66+l5FP/kCA0NFedKlB+hoaGV3tfFxQXfffcdfvrpJ3zzzTfIyclBjx49cOvWLeTk5AAA7Ozs1N5jZ2cnXsvJyYGJiQksLS2rjLG1ta1wb1tbW7WYp+9jaWkJExOTKmPKX5fHVAeHVIiISG9p8vH0wcHBCAwMVDv3ZK/AkwYNGiT+ul27dnB1dUXz5s2xadMmdO/eHUDFJbuCIDx3+OLpmMriNRFTPmG0JsuK2cNBRESkAXK5HBYWFmrHswqOp5mbm6Ndu3a4cOGCOK/j6d6D3NxcsWfB3t4eJSUlyMvLqzLm+vXrFe5148YNtZin75OXl4fS0tIqY3JzcwFU7IWpCgsOIiLSW5ocUvkniouLkZ6ejgYNGqBp06awt7fHwYMHxeslJSWIjY1Fjx49AABdunSBsbGxWkx2djbS0tLEGFdXV+Tn5+PEiRNizPHjx5Gfn68Wk5aWhuzsbDEmJiYGcrkcXbp0EWOOHj2qtlQ2JiYGKpUKTZo0qfZnrNaQyu7du6vdoJeXV7VjiYiIpCTVxl8zZszA0KFD0ahRI+Tm5uLzzz9HQUEBxo0bB5lMhoCAAISEhKBly5Zo2bIlQkJCYGZmBm9vbwCAUqnEhAkTEBQUhPr168PKygozZsxAu3btxFUrbdq0wcCBA+Hn54c1a9YAACZOnAhPT084OjoCAAYMGAAnJyf4+Phg8eLFuH37NmbMmAE/Pz9YWFgAeLy0dv78+fD19cXs2bNx4cIFhISEYM6cOTUqtKpVcAwfPrxajclkMrX1v0RERFRRVlYWxowZg5s3b8LGxgbdu3dHQkICGjduDAD46KOPUFRUhClTpiAvLw8uLi6IiYlB3bp1xTaWLl0KIyMjjBw5EkVFRejXrx82btwIQ0NDMWbr1q3w9/cXV7N4eXlh+fLl4nVDQ0Ps27cPU6ZMQc+ePaFQKODt7Y3w8HAxRqlU4uDBg5g6dSq6du0KS0tLBAYGVpiv8jwyoaZbhb0EFJ2mSZ0CkdblJS5/fhDRS85Uy0sb3tl2SmNtfefdXmNt1UZcpUJERHpLk6tUqGp/q+AoLCxEbGwsMjMzK+y37u/vr5HEiIiIqPaoccGRnJyMwYMH4/79+ygsLISVlRVu3rwJMzMz2NrasuAgIqKXBh9Przs1Xhb74YcfYujQobh9+zYUCgUSEhJw5coVdOnSRW2SCRER0YtOpsGDqlbjgiMlJQVBQUEwNDSEoaEhiouL4eDggEWLFmH27NnayJGIiIhecjUuOIyNjcUuKDs7O2RmZgJ4vGym/NdEREQvAykfT69vajyHo1OnTjh58iRatWqFPn36YM6cObh58yY2b96Mdu3aaSNHIiIirWCdoDs17uEICQlBgwYNAACfffYZ6tevj8mTJyM3Nxdr167VeIJERET08qtxD0fXrl3FX9vY2GD//v0aTYiIiEhXuEpFd7jxFxER6S3WG7pT44KjadOmVVaEly5d+kcJERERUe1T44IjICBA7XVpaSmSk5MRHR2NmTNnaiovIiIirePqEt2pccHxwQcfVHp+xYoVOHny5D9OiIiISFdYb+hOjVepPMugQYOwY8cOTTVHREREtYjGJo3++OOPsLKy0lRzREREWsdVKrrztzb+evI3SBAE5OTk4MaNG1i5cqVGk/u78hKXS50CERG9BDTWzU/PVeOCY9iwYWoFh4GBAWxsbNC7d2+0bt1ao8kRERFR7VDjgmPevHlaSIOIiEj3OKSiOzXuTTI0NERubm6F87du3YKhoaFGkiIiItIFA5nmDqpajQsOQRAqPV9cXAwTE5N/nBARERHVPtUeUlm2bBmAx91P3377LerUqSNeKysrw9GjRzmHg4iIXirsmdCdahccS5cuBfC4h2P16tVqwycmJiZo0qQJVq9erfkMiYiItIRzOHSn2gXH5cuXAQB9+vTBzp07YWlpqbWkiIiIqHap8SqVX375RRt5EBER6RyHVHSnxpNG33zzTYSFhVU4v3jxYrz11lsaSYqIiEgXZDLNHVS1GhccsbGxGDJkSIXzAwcOxNGjRzWSFBEREdUuNR5SuXfvXqXLX42NjVFQUKCRpIiIiHSBj6fXnRr3cDg7O+OHH36ocD4yMhJOTk4aSYqIiEgXDDR4UNVq3MPx6aefYsSIEbh48SL69u0LADh06BC2bduGH3/8UeMJEhER0cuvxgWHl5cXdu3ahZCQEPz4449QKBTo0KEDDh8+DAsLC23kSEREpBUcUdGdGhccADBkyBBx4uidO3ewdetWBAQE4I8//kBZWZlGEyQiItIWzuHQnb897HT48GGMHTsWKpUKy5cvx+DBg3Hy5ElN5kZERES1RI0KjqysLHz++edo1qwZxowZA0tLS5SWlmLHjh34/PPP0alTJ23lSUREpHEvwj4coaGhkMlkCAgIEM8JgoB58+ZBpVJBoVCgd+/eOH36tNr7iouLMX36dFhbW8Pc3BxeXl7IyspSi8nLy4OPjw+USiWUSiV8fHxw584dtZjMzEwMHToU5ubmsLa2hr+/P0pKStRiUlNT4ebmBoVCgYYNG2LBggXPfJjrs1S74Bg8eDCcnJxw5swZRERE4Nq1a4iIiKjRzYiIiF4kUj+ePjExEWvXrkX79u3Vzi9atAhLlizB8uXLkZiYCHt7e7i7u+Pu3btiTEBAAKKiohAZGYm4uDjcu3cPnp6ealMbvL29kZKSgujoaERHRyMlJQU+Pj7i9bKyMgwZMgSFhYWIi4tDZGQkduzYgaCgIDGmoKAA7u7uUKlUSExMREREBMLDw7FkyZIafVaZUM0SxcjICP7+/pg8eTJatmwpnjc2NsYff/zxQi2JffBQ6gyIiEgTTP/WTMPqmxdzQXNtDWj5/KAn3Lt3D507d8bKlSvx+eefo2PHjvjqq68gCAJUKhUCAgIwa9YsAI97M+zs7LBw4UJMmjQJ+fn5sLGxwebNmzFq1CgAwLVr1+Dg4ID9+/fDw8MD6enpcHJyQkJCAlxcXAAACQkJcHV1xdmzZ+Ho6IgDBw7A09MTV69ehUqlAvB4mwtfX1/k5ubCwsICq1atQnBwMK5fvw65XA4ACAsLQ0REBLKysqr9ALxq93D8+uuvuHv3Lrp27QoXFxcsX74cN27cqP43S0RE9IIxkMk0dhQXF6OgoEDtKC4ufua9p06diiFDhqB///5q5y9fvoycnBwMGDBAPCeXy+Hm5oZjx44BAJKSklBaWqoWo1Kp4OzsLMbEx8dDqVSKxQYAdO/eHUqlUi3G2dlZLDYAwMPDA8XFxUhKShJj3NzcxGKjPObatWvIyMio/ndd3UBXV1d88803yM7OxqRJkxAZGYmGDRvi0aNHOHjwoFo3DxER0ctAk3M4QkNDxbkS5UdoaGil942MjMTvv/9e6fWcnBwAgJ2dndp5Ozs78VpOTg5MTEwqPLn96RhbW9sK7dva2qrFPH0fS0tLmJiYVBlT/ro8pjpqvErFzMwM48ePR1xcHFJTUxEUFISwsDDY2trCy8urps0RERHVCsHBwcjPz1c7goODK8RdvXoVH3zwAbZs2QJTU9Nntvf0UIUgCM8dvng6prJ4TcSUz8ao7nAK8A93Y3V0dMSiRYuQlZWF77///p80RUREpHOanDQql8thYWGhdjw5DFEuKSkJubm56NKlC4yMjGBkZITY2FgsW7YMRkZGz+w9yM3NFa/Z29ujpKQEeXl5VcZcv369wv1v3LihFvP0ffLy8lBaWlplTG5uLoCKvTBV0cj274aGhhg+fDh2796tieaIiIh0QqbB/1VXv379kJqaipSUFPHo2rUr3n77baSkpKBZs2awt7fHwYMHxfeUlJQgNjYWPXr0AAB06dIFxsbGajHZ2dlIS0sTY1xdXZGfn48TJ06IMcePH0d+fr5aTFpaGrKzs8WYmJgYyOVydOnSRYw5evSo2lLZmJgYqFQqNGnSpNqfW8vzf4mIiOhJdevWhbOzs9o5c3Nz1K9fXzwfEBCAkJAQtGzZEi1btkRISAjMzMzg7e0NAFAqlZgwYQKCgoJQv359WFlZYcaMGWjXrp04CbVNmzYYOHAg/Pz8sGbNGgDAxIkT4enpCUdHRwDAgAED4OTkBB8fHyxevBi3b9/GjBkz4OfnJz6uxNvbG/Pnz4evry9mz56NCxcuICQkBHPmzKnRkAoLDiIi0lt/d/8Mbfvoo49QVFSEKVOmIC8vDy4uLoiJiUHdunXFmKVLl8LIyAgjR45EUVER+vXrh40bN8LQ0FCM2bp1K/z9/cXVLF5eXli+fLl43dDQEPv27cOUKVPQs2dPKBQKeHt7Izw8XIxRKpU4ePAgpk6diq5du8LS0hKBgYEIDAys0Weq9j4cLxPuw0FEVDtoex+ORb9c1FhbH/VprrG2aiONzOEgIiIiqgqHVIiISG/VZA4C/TMsOIiISG+9qHM4aiMOqRAREZHWsYeDiIj0FkdUdIcFBxER6S0DVhw6wyEVIiIi0jr2cBARkd7ipFHdYcFBRER6iyMqusMhFSIiItI69nAQEZHeMqjBU17pn2HBQUREeotDKrrDIRUiIiLSOvZwEBGR3uIqFd1hwUFERHqLG3/pDodUiIiISOvYw0FERHqLHRy6w4KDiIj0FodUdIdDKkRERKR17OEgIiK9xQ4O3WHBQUREeovd/LrD75qIiIi0jj0cRESkt2QcU9EZFhxERKS3WG7oDodUiIiISOvYw0FERHqL+3DoDgsOIiLSWyw3dIdDKkRERKR17OEgIiK9xREV3WHBQUREeovLYnWHQypERESkdezhICIivcV/desOv2siItJbMplMY0dNrFq1Cu3bt4eFhQUsLCzg6uqKAwcOiNcFQcC8efOgUqmgUCjQu3dvnD59Wq2N4uJiTJ8+HdbW1jA3N4eXlxeysrLUYvLy8uDj4wOlUgmlUgkfHx/cuXNHLSYzMxNDhw6Fubk5rK2t4e/vj5KSErWY1NRUuLm5QaFQoGHDhliwYAEEQajRZ2bBQUREpGOvvPIKwsLCcPLkSZw8eRJ9+/bFsGHDxKJi0aJFWLJkCZYvX47ExETY29vD3d0dd+/eFdsICAhAVFQUIiMjERcXh3v37sHT0xNlZWVijLe3N1JSUhAdHY3o6GikpKTAx8dHvF5WVoYhQ4agsLAQcXFxiIyMxI4dOxAUFCTGFBQUwN3dHSqVComJiYiIiEB4eDiWLFlSo88sE2paorwEHjyUOgMiItIEUy0P/P8n5ZrG2nqro+ofvd/KygqLFy/G+PHjoVKpEBAQgFmzZgF43JthZ2eHhQsXYtKkScjPz4eNjQ02b96MUaNGAQCuXbsGBwcH7N+/Hx4eHkhPT4eTkxMSEhLg4uICAEhISICrqyvOnj0LR0dHHDhwAJ6enrh69SpUqsf5R0ZGwtfXF7m5ubCwsMCqVasQHByM69evQy6XAwDCwsIQERGBrKysavfuSN7DcfXqVbUuoBMnTiAgIABr166VMCsiItIHmhxSKS4uRkFBgdpRXFz83BzKysoQGRmJwsJCuLq64vLly8jJycGAAQPEGLlcDjc3Nxw7dgwAkJSUhNLSUrUYlUoFZ2dnMSY+Ph5KpVIsNgCge/fuUCqVajHOzs5isQEAHh4eKC4uRlJSkhjj5uYmFhvlMdeuXUNGRka1v2vJCw5vb2/88ssvAICcnBy4u7vjxIkTmD17NhYsWCBxdkRERNUTGhoqzpUoP0JDQ58Zn5qaijp16kAul+P9999HVFQUnJyckJOTAwCws7NTi7ezsxOv5eTkwMTEBJaWllXG2NraVrivra2tWszT97G0tISJiUmVMeWvy2OqQ/KCIy0tDd26dQMAbN++XazOtm3bho0bN0qbHBER1WoGGjyCg4ORn5+vdgQHBz/z3o6OjkhJSUFCQgImT56McePG4cyZM+L1p4cqBEF47vDF0zGVxWsipnw2Rk0my0pecJSWlordND///DO8vLwAAK1bt0Z2draUqRERUS2nySEVuVwurjopP54chniaiYkJWrRoga5duyI0NBQdOnTA119/DXt7ewAVew9yc3PFngV7e3uUlJQgLy+vypjr169XuO+NGzfUYp6+T15eHkpLS6uMyc3NBVCxF6Yqkhccbdu2xerVq/Hrr7/i4MGDGDhwIIDHk1/q168vcXZERES6IQgCiouL0bRpU9jb2+PgwYPitZKSEsTGxqJHjx4AgC5dusDY2FgtJjs7G2lpaWKMq6sr8vPzceLECTHm+PHjyM/PV4tJS0tT+wd+TEwM5HI5unTpIsYcPXpUbalsTEwMVCoVmjRpUu3PJ3nBsXDhQqxZswa9e/fGmDFj0KFDBwDA7t27xaEWIiIibZBp8KiJ2bNn49dff0VGRgZSU1PxySef4MiRI3j77bchk8kQEBCAkJAQREVFIS0tDb6+vjAzM4O3tzcAQKlUYsKECQgKCsKhQ4eQnJyMsWPHol27dujfvz8AoE2bNhg4cCD8/PyQkJCAhIQE+Pn5wdPTE46OjgCAAQMGwMnJCT4+PkhOTsahQ4cwY8YM+Pn5wcLCAsDjuZZyuRy+vr5IS0tDVFQUQkJCEBgYWKMhFcl3Gu3duzdu3ryJgoICtckvEydOhJmZmYSZERFRbSfVo1SuX78OHx8fZGdnQ6lUon379oiOjoa7uzsA4KOPPkJRURGmTJmCvLw8uLi4ICYmBnXr1hXbWLp0KYyMjDBy5EgUFRWhX79+2LhxIwwNDcWYrVu3wt/fX1zN4uXlheXLl4vXDQ0NsW/fPkyZMgU9e/aEQqGAt7c3wsPDxRilUomDBw9i6tSp6Nq1KywtLREYGIjAwMAafWbuw0FERC8sbe/D8d/U6q+yeJ5h7ew11lZtJHkPR9OmTavskrl06ZIOsyEiIn1iUOPBEPq7JC84AgIC1F6XlpYiOTkZ0dHRmDlzpjRJERGRXuDT6XVH8oLjgw8+qPT8ihUrcPLkSR1nQ0RERNog+SqVZxk0aBB27NghdRpERFSLyTT4P6qa5D0cz/Ljjz/CyspK6jSIiKgW45CK7khecHTq1Elt0qggCMjJycGNGzewcuVKCTMjIiIiTZG84Bg+fLjaawMDA9jY2KB3795o3bq1NEkREZFe4CoV3eE+HERE9MLS9j4cP525obG2PJxsNNZWbSR5DwcAlJWVYdeuXUhPT4dMJoOTkxO8vLzUdksjIiKil5fkBceff/6JwYMH43//+x8cHR0hCALOnz8PBwcH7Nu3D82bN5c6RSIiqqU4aVR3JF8W6+/vj+bNm+Pq1av4/fffkZycjMzMTDRt2hT+/v5Sp0dERLUYl8XqjuRzOMzNzZGQkIB27dqpnf/jjz/Qs2dP3Lt3r8Ztcg4HEVHtoO05HAfTb2qsLfc21hprqzaSfEhFLpfj7t27Fc7fu3cPJiYmEmRERET6woAdEzoj+ZCKp6cnJk6ciOPHj0MQBAiCgISEBLz//vvw8vKSOj0iIqrFOKSiO5IXHMuWLUPz5s3h6uoKU1NTmJqaomfPnmjRogW+/vprqdMjIiIiDZB8Dke5Cxcu4OzZsxAEAU5OTmjRosXfbotzOIiIagdtz+H45dwtjbXVx7G+xtqqjSSfw1GuZcuWaNmypdRpEBGRHuFQiO5IUnAEBgbis88+g7m5OQIDA6uMXbJkiY6yIiIiIm2RpOBITk5GaWmp+OtnkXFHFiIi0iKuUtGdF2YOhyZxDgcRUe2g7Tkcv57P01hbvVpZaqyt2kjyORz5+fkoKyuDlZWV2vnbt2/DyMgIFhYWEmVG169fx1dLFuO3X39FcfEDNG7cBPM++wJObZ3FmEsXL+KrJYuRdDIRjx49QvMWLbH4y6/QQKUCAJSUlODLxQsRvX8vHhQXw8WlOz75dB7s7O2l+lhEonXfrMGhgzG4fPkS5Kam6NixEwICZ6BJ02ZiTIe2jpW+98OgmfAd/x4AYIKvD04mnlC77jFoMBaFL9Ve8kQvGckLjtGjR2Po0KGYMmWK2vnt27dj9+7d2L9/v0SZ6beC/Hz4jh2Drt1csGL1N7Cqb4Wsq1dRt+5fBeDVzEz4+njjX2+MwORp/qhbpy4uXboIE7lcjFkU9gVij/yCheFLoaxXD18uCsP0KZPw/X928uF8JLmTiScwaszbaNuuHcoeliFi2VK87zcBO3fvg5mZGQDg0JE4tffExR3FvE8/QX93D7XzI94ciSnT/nocg9zUVPsfgP4xjtzrjuRDKlZWVvjtt9/Qpk0btfNnz55Fz549cetWzZcscUjln/tqSThSkn/Hxs3bnhnz0YwPYWRkhJCwxZVev3v3Lnq/5oovwhZh4KDBAIDc3Ovw6Ncby1etRc/Xemkld6K/6/bt2+jTyxXrN21Bl66vVhoTMH0KCgsL8c36TeK5Cb4+cHRsjY+CP9FVqnpD20Mqv13Q3JBKz5YcUqmK5Bt/FRcX4+HDihVCaWkpioqKJMiIACD2l8No29YZMz70R+9erhg5Yjh2/Ge7eP3Ro0f4NfYIGjdugvf9JqB3L1e8PfotHD70sxhz5nQaHj4sRY8ePcVztrZ2aNGiJf5IefZkYSKp3Pv/xyxYKJWVXr918yZ+PRqLf73xZoVr+/ftgVtPF/zLawi+XLwQhYU1fw4UUW0mecHx6quvYu3atRXOr169Gl26dHnu+4uLi1FQUKB2FBcXayNVvZKVdRXbf/gejRo3waq16/DWqNFYGPo59vx3FwDg9q1buH//Ptav+wY9X+uF1WvXo28/dwR+ME0cy7518yaMjY0r/MfbytoaN29q7oFJRJogCALCF4WiU+cuaNmyVaUxu/8bBTMzc/RzH6B2fvCQoQhbvATfbtyMie9Pwc8Hf0LgB9N1kTb9QwYymcYOqprkczi++OIL9O/fH3/88Qf69esHADh06BASExMRExPz3PeHhoZi/vz5auc++XQu/j1nnjbS1RuPHglo6+wM/4DH+6S0aeOEi3/+ie0/fI+hw4bjkfAIANCnTz/4jPMFALRu0wZ/pPyO//wQia6vdnt244LAcVN64YR+vgAXzp+vchhxV9QODPYcCvkT85QAYMRbI8Vft2zZCo0bN8aYkSOQfuY02ji11VrO9M/xP0W6I3kPR8+ePREfHw8HBwds374de/bsQYsWLXDq1Cn06vX8Mf7g4GDk5+erHTNnBesg89rNxsYGzZo3VzvXrFkzZGdfAwBY1rOEkZFRhZimzZoj5/9j6ltbo7S0FAX5+Woxt2/dQv36fIwzvThCv/gMR44cxjcbNj1zBdXvSSeRcfky3hjx1nPba+PUFkZGxrhy5YqmUyV6aUnewwEAHTt2xNatW//We+VyeYV/bXDS6D/XsVNnZFy+rHbuSkYGVKqGAABjExO0dW6HjIynYq5koMH/xzi1dYaRkTHi43+Dx8DHk0Zv3MjFn39eQEDQTB18CqKqCYKA0C8+w+FDB7Fu42a88orDM2OjdvwIp7Zt4di69XPb/fPPC3j4sBQ2NjaaTJe0gV0cOiNJwVFQUCDur1FQUFBlLPfhkMbYd8Zh3Ngx+HbtagzwGIS01FP48cftmDNvgRgz7t0J+CjoQ3Tp8ipe7eaC3+J+xdEjv+DbDd8BAOrWrYt/jRiBLxcvRL16lrBQKrFk8UK0bNkK3V17SPXRiEQhn83Hgf178VXESpibmePmjRsAgDp168L0iWWt9+7dQ0xMNIJmzqrQxtXMTOzbuxu9XndDPUtLXLp4EV8uDkPrNk7o2Kmzzj4L/T18loruSLIs1tDQENnZ2bC1tYWBgUGlW5gLggCZTIaysrIat88eDs2IPfILln21BJlXMtDwlVfg8867amPVABC180es/2Ytrl/PQZMmTTF52nT06dtfvF5cXIwl4YtwYN9eFBc/QDcXV3zy6VzYN2ig649DVMGzNvVa8Hkohv3rDfH1j9t/wOKFIfj5SBzq1q2rFpuTnY3ZH8/Enxcu4P79QtjbN0AvNze8P3kalPXqaTN9vaDtZbHHL+Y/P6iaXJpXvrqJHpOk4IiNjUXPnj1hZGSE2NjYKmPd3Nxq3D4LDiKi2kHbBceJS5orOLo1Y8FRFck3/tIGFhxERLWDtguORA0WHK+y4KiS5KtUACAvLw/h4eGYMGEC3nvvPXz55Ze4ffu21GkRERFpRWhoKF599VXUrVsXtra2GD58OM6dO6cWIwgC5s2bB5VKBYVCgd69e+P06dNqMcXFxZg+fTqsra1hbm4OLy8vZGVlqcXk5eXBx8cHSqUSSqUSPj4+uHPnjlpMZmYmhg4dCnNzc1hbW8Pf3x8lJSVqMampqXBzc4NCoUDDhg2xYMEC1KTPQvKCIzY2Fk2aNMGyZcuQl5eH27dvY9myZWjatOlzh1uIiIj+EZkGjxqIjY3F1KlTkZCQgIMHD+Lhw4cYMGAACgsLxZhFixZhyZIlWL58ORITE2Fvbw93d3fc/f8dcQEgICAAUVFRiIyMRFxcHO7duwdPT0+1+Y/e3t5ISUlBdHQ0oqOjkZKSAh8fH/F6WVkZhgwZgsLCQsTFxSEyMhI7duxAUFCQGFNQUAB3d3eoVCokJiYiIiIC4eHhWLJkSbU/s+RDKs7OzujRowdWrVolPsyrrKwMU6ZMwW+//Ya0tLQat8khFSKi2kHbQyonL1e9UrImujb9+6sqb9y4AVtbW8TGxuL111+HIAhQqVQICAjArFmPV0cVFxfDzs4OCxcuxKRJk5Cfnw8bGxts3rwZo0aNAgBcu3YNDg4O2L9/Pzw8PJCeng4nJyckJCTAxcUFAJCQkABXV1ecPXsWjo6OOHDgADw9PXH16lWo/v9J35GRkfD19UVubi4sLCywatUqBAcH4/r16+JWFGFhYYiIiEBWVlaliz+eJnkPx8WLFxEUFKT25FBDQ0MEBgbi4sWLEmZGRERUff/kURv5/79BopWVFQDg8uXLyMnJwYABf22jL5fL4ebmhmPHjgEAkpKSUFpaqhajUqng7OwsxsTHx0OpVIrFBgB0794dSqVSLcbZ2VksNgDAw8MDxcXFSEpKEmPc3NzU9r3y8PDAtWvXkJGRUa3PKHnB0blzZ6Snp1c4n56ejo4dO+o+ISIi0hsymeaO0NBQcZ5E+REaGvrcHARBQGBgIF577TU4OzsDAHJycgAAdnZ2arF2dnbitZycHJiYmMDS0rLKGFtb2wr3tLW1VYt5+j6WlpYwMTGpMqb8dXnM80i+06i/vz8++OAD/Pnnn+jevTuAx909K1asQFhYGE6dOiXGtm/fXqo0iYiIqhQcHIzAwEC1c0/vhF2ZadOm4dSpU4iLi6tw7emhivI9qqrydExVe139k5jyGRnVGU4BXoCCY8yYMQCAjz76qNJrMpnsH20CRkRE9Cya3Ge0skdtPM/06dOxe/duHD16FK+88op43v7/n+mTk5ODBk9slJibmyv2LNjb26OkpAR5eXlqvRy5ubno0aOHGHP9+vUK971x44ZaO8ePH1e7npeXh9LSUrWYp3sycnNzAVTshXkWyYdULl++XOVx6dIl8f+JiIg0SqJVKoIgYNq0adi5cycOHz6Mpk2bql1v2rQp7O3tcfDgQfFcSUkJYmNjxWKiS5cuMDY2VovJzs5GWlqaGOPq6or8/HycOHFCjDl+/Djy8/PVYtLS0pCdnS3GxMTEQC6Xo0uXLmLM0aNH1ZbKxsTEQKVSoUmTJtX6zJKvUtEGrlIhIqodtL1K5fcrmlul0rlx9VepTJkyBdu2bcN///tfODr+tcW+UqmEQqEAACxcuBChoaHYsGEDWrZsiZCQEBw5cgTnzp0Tt9ifPHky9u7di40bN8LKygozZszArVu3kJSUJC7GGDRoEK5du4Y1a9YAACZOnIjGjRtjz549AB6vDO3YsSPs7OywePFi3L59G76+vhg+fDgiIiIAPJ7U6ujoiL59+2L27Nm4cOECfH19MWfOHLXls1WRvODYtGkTrK2tMWTIEACPh1bWrl0LJycnfP/992jcuHGN22TBQURUO2i74Ei+cvf5QdXUqXHd5wf9v2fNe9iwYQN8fX0BPO4FmT9/PtasWYO8vDy4uLhgxYoV4sRSAHjw4AFmzpyJbdu2oaioCP369cPKlSvh4PDXk49v374Nf39/7N69GwDg5eWF5cuXo94Tz/rJzMzElClTcPjwYSgUCnh7eyM8PFxtiCg1NRVTp07FiRMnYGlpiffffx9z5syp9hwOyQsOR0dHrFq1Cn379kV8fDz69euHr776Cnv37oWRkRF27txZ4zZZcBAR1Q7aLjhSMjVXcHRsVP2CQx9JPmn06tWraNGiBQBg165dePPNNzFx4kT07NkTvXv3ljY5IiIi0gjJJ43WqVMHt27dAvB4Akr//o8fbW5qaoqioiIpUyMiolpOojmjeknyHg53d3e899576NSpE86fPy/O5Th9+nS1Z74SERH9LawUdEbyHo4VK1bA1dUVN27cwI4dO1C/fn0Aj7dsLd+jg4iIiF5ukk8a1QZOGiUiqh20PWn01NV7GmurvUMdjbVVG0k+pHL06NEqr7/++us6yoSIiPRNNVd0kgZIXnBUthLlyTW93M6ciIjo5Sf5HI68vDy1Izc3F9HR0Xj11VcRExMjdXpERFSLcZWK7kjew6FUKiucc3d3h1wux4cffoikpCQJsiIiIr3ASkFnJO/heBYbGxucO3dO6jSIiIhIAyTv4Th16pTaa0EQkJ2djbCwMHTo0EGirIiISB/I2MWhM5IXHB07doRMJsPTq3O7d++O9evXS5QVERHpA65S0R3JC47Lly+rvTYwMICNjQ1MTU0lyoiIiIg0TbI5HMePH8eBAwfQuHFj8YiNjcXrr7+ORo0aYeLEiSguLpYqPSIi0gNcpaI7khUc8+bNU5u/kZqaigkTJqB///74+OOPsWfPHoSGhkqVHhER6QNWHDojWcGRkpKCfv36ia8jIyPh4uKCb775BoGBgVi2bBm2b98uVXpERESkQZLN4cjLy4OdnZ34OjY2FgMHDhRfv/rqq7h69aoUqRERkZ7gKhXdkayHw87OTpwwWlJSgt9//x2urq7i9bt378LY2Fiq9IiISA/IZJo7qGqSFRwDBw7Exx9/jF9//RXBwcEwMzNDr169xOunTp1C8+bNpUqPiIiINEiyIZXPP/8cb7zxBtzc3FCnTh1s2rQJJiYm4vX169djwIABUqVHRER6gB0TuiMTnt5xS8fy8/NRp04dGBoaqp2/ffs26tSpo1aEVNeDh5rKjoiIpGSq5X8Wn79+X2NttbIz01hbtZHkG39V9vA2ALCystJxJkRERKQtkhccREREUuEqFd1hwUFERHqLq0t054V9PD0RERHVHuzhICIivcUODt1hwUFERPqLFYfOcEiFiIiItI49HEREpLe4SkV3WHAQEZHe4ioV3eGQChEREWkdeziIiEhvsYNDd9jDQURE+kumwaMGjh49iqFDh0KlUkEmk2HXrl1q1wVBwLx586BSqaBQKNC7d2+cPn1aLaa4uBjTp0+HtbU1zM3N4eXlhaysLLWYvLw8+Pj4QKlUQqlUwsfHB3fu3FGLyczMxNChQ2Fubg5ra2v4+/ujpKRELSY1NRVubm5QKBRo2LAhFixYgJo+io0FBxERkY4VFhaiQ4cOWL58eaXXFy1ahCVLlmD58uVITEyEvb093N3dcffuXTEmICAAUVFRiIyMRFxcHO7duwdPT0+UlZWJMd7e3khJSUF0dDSio6ORkpICHx8f8XpZWRmGDBmCwsJCxMXFITIyEjt27EBQUJAYU1BQAHd3d6hUKiQmJiIiIgLh4eFYsmRJjT6z5E+L1QY+LZaIqHbQ9tNir9wq1lhbjevL/9b7ZDIZoqKiMHz4cACPezdUKhUCAgIwa9YsAI97M+zs7LBw4UJMmjQJ+fn5sLGxwebNmzFq1CgAwLVr1+Dg4ID9+/fDw8MD6enpcHJyQkJCAlxcXAAACQkJcHV1xdmzZ+Ho6IgDBw7A09MTV69ehUqlAgBERkbC19cXubm5sLCwwKpVqxAcHIzr169DLn/8GcPCwhAREYGsrCzIqjnzlj0cRESkt2QyzR3FxcUoKChQO4qLa17QXL58GTk5ORgwYIB4Ti6Xw83NDceOHQMAJCUlobS0VC1GpVLB2dlZjImPj4dSqRSLDQDo3r07lEqlWoyzs7NYbACAh4cHiouLkZSUJMa4ubmJxUZ5zLVr15CRkVHtz8WCg4iISANCQ0PFuRLlR2hoaI3bycnJAQDY2dmpnbezsxOv5eTkwMTEBJaWllXG2NraVmjf1tZWLebp+1haWsLExKTKmPLX5THVwVUqRESktzS5SiU4OBiBgYFq557sFaipp4cqBEF47vDF0zGVxWsipnw2RnWHUwD2cBARkR7T5JCKXC6HhYWF2vF3Cg57e3sAFXsPcnNzxZ4Fe3t7lJSUIC8vr8qY69evV2j/xo0bajFP3ycvLw+lpaVVxuTm5gKo2AtTFRYcREREL5CmTZvC3t4eBw8eFM+VlJQgNjYWPXr0AAB06dIFxsbGajHZ2dlIS0sTY1xdXZGfn48TJ06IMcePH0d+fr5aTFpaGrKzs8WYmJgYyOVydOnSRYw5evSo2lLZmJgYqFQqNGnSpNqfiwUHERHpMWk24rh37x5SUlKQkpIC4PFE0ZSUFGRmZkImkyEgIAAhISGIiopCWloafH19YWZmBm9vbwCAUqnEhAkTEBQUhEOHDiE5ORljx45Fu3bt0L9/fwBAmzZtMHDgQPj5+SEhIQEJCQnw8/ODp6cnHB0dAQADBgyAk5MTfHx8kJycjEOHDmHGjBnw8/ODhYUFgMdLa+VyOXx9fZGWloaoqCiEhIQgMDCwRkMqXBZLREQvLG0vi/3fnZLnB1VTw3om1Y49cuQI+vTpU+H8uHHjsHHjRgiCgPnz52PNmjXIy8uDi4sLVqxYAWdnZzH2wYMHmDlzJrZt24aioiL069cPK1euhIODgxhz+/Zt+Pv7Y/fu3QAALy8vLF++HPXq1RNjMjMzMWXKFBw+fBgKhQLe3t4IDw9XGw5KTU3F1KlTceLECVhaWuL999/HnDlzWHCw4CAiqh1qa8Ghj7hKhYiI9BafpaI7LDiIiEhv8fH0usNJo0RERKR17OEgIiK9JeOgis6w4CAiIv3FekNnOKRCREREWsceDiIi0lvs4NAdFhxERKS3uEpFdzikQkRERFrHHg4iItJbXKWiOyw4iIhIf7He0BkOqRAREZHWsYeDiIj0Fjs4dIcFBxER6S2uUtEdDqkQERGR1rGHg4iI9BZXqegOCw4iItJbHFLRHQ6pEBERkdax4CAiIiKt45AKERHpLQ6p6A57OIiIiEjr2MNBRER6i6tUdIcFBxER6S0OqegOh1SIiIhI69jDQUREeosdHLrDgoOIiPQXKw6d4ZAKERERaR17OIiISG9xlYrusOAgIiK9xVUqusMhFSIiItI69nAQEZHeYgeH7rDgICIi/cWKQ2c4pEJERERaxx4OIiLSW1ylojssOIiISG9xlYrucEiFiIiItE4mCIIgdRL0cisuLkZoaCiCg4Mhl8ulTodIK/hzTvTPsOCgf6ygoABKpRL5+fmwsLCQOh0ireDPOdE/wyEVIiIi0joWHERERKR1LDiIiIhI61hw0D8ml8sxd+5cTqSjWo0/50T/DCeNEhERkdaxh4OIiIi0jgUHERERaR0LDiIiItI6FhwkiXnz5qFjx47VjpfJZNi1a5fW8iGqypEjRyCTyXDnzp1qxffu3RsBAQFazYnoZcOCQ2K+vr6QyWQICwtTO79r1y7IavhUoSZNmuCrr756blxycjI8PT1ha2sLU1NTNGnSBKNGjcLNmzdrdL9/YsaMGTh06FC147OzszFo0CAtZkQvstzcXEyaNAmNGjWCXC6Hvb09PDw8EB8fr5P79+jRA9nZ2VAqldWK37lzJz777DMtZ0X0cuHTYl8ApqamWLhwISZNmgRLS0ut3is3Nxf9+/fH0KFD8dNPP6FevXq4fPkydu/ejfv372v13k+qU6cO6tSpU+14e3t7LWZDL7oRI0agtLQUmzZtQrNmzXD9+nUcOnQIt2/f1sn9TUxMavQzaGVlpcVsiF5SAklq3Lhxgqenp9C6dWth5syZ4vmoqCjh6d+eH3/8UXBychJMTEyExo0bC+Hh4eI1Nzc3AYDaUZmoqCjByMhIKC0tfWZOGzZsEJRKZYX3PdlmSkqK0Lt3b6FOnTpC3bp1hc6dOwuJiYlq74+KihJatmwpyOVyoX///kJmZqb4/rlz5wodOnRQu8e6devEz2dvby9MnTpVvAZAiIqKEl+fOnVK6NOnj2BqaipYWVkJfn5+wt27d9W+jw8++ECt/WHDhgnjxo0TX69YsUJo0aKFIJfLBVtbW2HEiBHP/E5IOnl5eQIA4ciRI5Vev3z5sgBASE5OrvCeX375RRAEQbh9+7bg7e0tWFtbC6ampkKLFi2E9evXq73/+++/F1xdXQW5XC44OTmJ7xUEQfjll18EAEJeXp54Li4uTnj99dcFhUIh1KtXTxgwYIBw+/ZtQRAq/vzdvn1b8PHxEerVqycoFAph4MCBwvnz58Xrlf15WLp0qdC4cWO1HF599VXBzMxMUCqVQo8ePYSMjIzqf5FEEuOQygvA0NAQISEhiIiIQFZWVqUxSUlJGDlyJEaPHo3U1FTMmzcPn376KTZu3AjgcRfuK6+8ggULFiA7OxvZ2dmVtmNvb4+HDx8iKioKwj/YguXtt9/GK6+8gsTERCQlJeHjjz+GsbGxeP3+/fv44osvsGnTJvz2228oKCjA6NGjn9neqlWrMHXqVEycOBGpqanYvXs3WrRoUWns/fv3MXDgQFhaWiIxMRH/+c9/8PPPP2PatGnVzv/kyZPw9/fHggULcO7cOURHR+P111+v/hdAOlPeG7Zr1y4UFxf/rTY+/fRTnDlzBgcOHEB6ejpWrVoFa2trtZiZM2ciKCgIycnJ6NGjB7y8vHDr1q1K20tJSUG/fv3Qtm1bxMfHIy4uDkOHDkVZWVml8b6+vjh58iR2796N+Ph4CIKAwYMHo7S0tFr5P3z4EMOHD4ebmxtOnTqF+Ph4TJw4scbDrkSSkrri0Xfjxo0Thg0bJgiCIHTv3l0YP368IAgVexS8vb0Fd3d3tffOnDlTcHJyEl83btxYWLp06XPvOXv2bMHIyEiwsrISBg4cKCxatEjIyckRr1enh6Nu3brCxo0bK21/w4YNAgAhISFBPJeeni4AEI4fPy4IQsV/0alUKuGTTz55Zs54oodj7dq1gqWlpXDv3j3x+r59+wQDAwPxczyvh2PHjh2ChYWFUFBQ8Mx70ovjxx9/FCwtLQVTU1OhR48eQnBwsPDHH38IglC9Ho6hQ4cK7777bqVtl78/LCxMPFdaWiq88sorwsKFCwVBqNjDMWbMGKFnz57PzPfJn7/z588LAITffvtNvH7z5k1BoVAI27dvFwTh+T0ct27dqrKXh+hlwB6OF8jChQuxadMmnDlzpsK19PR09OzZU+1cz549ceHChWf+q+pZvvjiC+Tk5GD16tVwcnLC6tWr0bp1a6Smpla7jcDAQLz33nvo378/wsLCcPHiRbXrRkZG6Nq1q/i6devWqFevHtLT0yu0lZubi2vXrqFfv37Vund6ejo6dOgAc3Nz8VzPnj3x6NEjnDt3rlptuLu7o3HjxmjWrBl8fHywdetWnc5hoZoZMWIErl27ht27d8PDwwNHjhxB586dxR6+55k8eTIiIyPRsWNHfPTRRzh27FiFGFdXV/HX5T+/lf28An/1cFRHeno6jIyM4OLiIp6rX78+HB0dn9n+06ysrODr6wsPDw8MHToUX3/99TN7MYleVCw4XiCvv/46PDw8MHv27ArXBEGo0H0q/IMhkfr16+Ott97Cl19+ifT0dKhUKoSHhwMADAwMKrT9dNfvvHnzcPr0aQwZMgSHDx+Gk5MToqKi1GIq6+6t7JxCoahR7pV9F0+3/7zPULduXfz+++/4/vvv0aBBA8yZMwcdOnSo9rJH0j1TU1O4u7tjzpw5OHbsGHx9fTF37lwYGDz+z9iTv99P/7wOGjQIV65cQUBAgFjczpgx47n3fNbPWU1+Zp/15/TJn+Pq/JnbsGED4uPj0aNHD/zwww9o1aoVEhISqp0HkdRYcLxgwsLCsGfPngr/AnNyckJcXJzauWPHjqFVq1YwNDQE8HgmfU17O8rf17x5cxQWFgIAbGxscPfuXfE18PhfdE9r1aoVPvzwQ8TExOCNN97Ahg0bxGsPHz7EyZMnxdfnzp3DnTt30Lp16wrt1K1bF02aNKn2MlknJyekpKSo5ffbb7/BwMAArVq1Ej/Dk/8CLCsrQ1pamlo7RkZG6N+/PxYtWoRTp04hIyMDhw8frlYOJD0nJycUFhbCxsYGANR+vyv7ebWxsYGvry+2bNmCr776CmvXrlW7/uRf3g8fPkRSUlKlP68A0L59+xr9vD58+BDHjx8Xz926dQvnz59HmzZtxNxycnLUio7KPkOnTp0QHByMY8eOwdnZGdu2batWDkQvBKnGcuixJ+dwlPPx8RFMTU3V5kwkJSUJBgYGwoIFC4Rz584JGzduFBQKhbBhwwYxxt3dXfDy8hKysrKEGzduVHq/PXv2CG+//bawZ88e4dy5c8LZs2eFxYsXC4aGhsJ3330nCMLj8WJzc3PB399fuHDhgrB161ZBpVKJ+dy/f1+YOnWq8MsvvwgZGRlCXFyc0Lx5c+Gjjz4SBOHxHA5jY2OhW7duQkJCgpCUlCS4uroK3bt3F/N4esx648aNgqmpqfD1118L58+fF5KSkoRly5aJ1/HEHI7CwkKhQYMGwogRI4TU1FTh8OHDQrNmzdRWoKxevVowMzMT9u7dK6SnpwsTJ04ULCwsxJg9e/YIX3/9tZCcnCxkZGQIK1euFAwMDIS0tLRq/b6R7ty8eVPo06ePsHnzZuGPP/4QLl26JGzfvl2ws7MT5zx1795d6NWrl3D69GkhNjZW6Natm9ocjk8//VTYtWuXcOHCBSEtLU3w9PQUunXrJgjCX3M4GjVqJOzcuVP8ealTp4745+jpORznzp0TTExMhMmTJwt//PGHkJ6eLqxcuVKMf3oO0bBhwwQnJyfh119/FVJSUoSBAwcKLVq0EEpKSgRBEIQzZ84IMplMCAsLE/78809h+fLlgqWlpTiH49KlS8LHH38sHDt2TMjIyBB++uknwcrKSli5cqWWv30izWHBIbHKCo6MjAxBLpc/c1mssbGx0KhRI2Hx4sVq1+Pj44X27dtX+t5yFy9eFPz8/IRWrVqJy/leffVVtcJFEB5PEm3RooVgamoqeHp6CmvXrhXbLC4uFkaPHi04ODgIJiYmgkqlEqZNmyYUFRUJgvDXpNMdO3YIzZo1E0xMTIS+ffuqLeGrbJLc6tWrBUdHR8HY2Fho0KCBMH36dPHakwWHIDx/WWxJSYkwefJkwcrKSrC1tRVCQ0PVJo3++uuvgpubm2BpaSkoFAqhffv2wg8//FDpd0bSevDggfDxxx8LnTt3FpRKpWBmZiY4OjoK//73v4X79+8LgvD4L+zu3bsLCoVC6NixoxATE6NWcHz22WdCmzZtBIVCIVhZWQnDhg0TLl26JAjCXwXHtm3bBBcXF8HExERo06aNcOjQITGHypbFHjlyROjRo4cgl8uFevXqCR4eHuL1Zy2LVSqVgkKhEDw8PNSWxQqCIKxatUpwcHAQzM3NhXfeeUf44osvxIIjJydHGD58uNCgQQNxWfycOXOEsrIyzX7ZRFrEx9OTxm3cuBEBAQGcD0EvhYyMDDRt2hTJyck12m6fiGqGcziIiIhI61hwEBERkdZxSIWIiIi0jj0cREREpHUsOIiIiEjrWHAQERGR1rHgICIiIq1jwUFERERax4KD6CUwb948tU2pfH19MXz4cJ3nkZGRAZlMVulzPoiIqsKCg+gf8PX1hUwmg0wmg7GxMZo1a4YZM2aoPVhOG77++utqP5qdRQIRvQiMpE6A6GU3cOBAbNiwAaWlpfj111/x3nvvobCwEKtWrVKLKy0thbGxsUbuqVQqNdIOEZGusIeD6B+Sy+Wwt7eHg4MDvL298fbbb2PXrl3iMMj69evRrFkzyOVyCIKA/Px8TJw4Eba2trCwsEDfvn3xxx9/qLUZFhYGOzs71K1bFxMmTMCDBw/Urj89pPLo0SMsXLgQLVq0gFwuR6NGjfDFF18AAJo2bQrg8aPNZTIZevfuLb5vw4YNaNOmDUxNTdG6dWusXLlS7T4nTpxAp06dYGpqiq5duyI5OVmD3xwR6RP2cBBpmEKhQGlpKQDgzz//xPbt27Fjxw4YGhoCAIYMGQIrKyvs378fSqUSa9asQb9+/XD+/HlYWVlh+/btmDt3LlasWIFevXph8+bNWLZsGZo1a/bMewYHB+Obb77B0qVL8dprryE7Oxtnz54F8Lho6NatG37++We0bdsWJiYmAIBvvvkGc+fOxfLly9GpUyckJyfDz88P5ubmGDduHAoLC+Hp6Ym+fftiy5YtuHz5Mj744AMtf3tEVGtJ+qxaopfcuHHjhGHDhomvjx8/LtSvX18YOXKkMHfuXMHY2FjIzc0Vrx86dEiwsLAQHjx4oNZO8+bNhTVr1giCIAiurq7C+++/r3bdxcVF6NChQ6X3LSgoEORyufDNN99UmmP549eTk5PVzjs4OAjbtm1TO/fZZ58Jrq6ugiAIwpo1awQrKyuhsLBQvL5q1apK2yIieh4OqRD9Q3v37kWdOnVgamoKV1dXvP7664iIiAAANG7cGDY2NmJsUlIS7t27h/r166NOnTricfnyZVy8eBEAkJ6eDldXV7V7PP36Senp6SguLka/fv2qnfONGzdw9epVTJgwQS2Pzz//XC2PDh06wMzMrFp5EBFVhUMqRP9Qnz59sGrVKhgbG0OlUqlNDDU3N1eLffToERo0aIAjR45UaKdevXp/6/4KhaLG73n06BGAx8MqLi4uatfKh34EPteRiDSIBQfRP2Rubo4WLVpUK7Zz587IycmBkZERmjRpUmlMmzZtkJCQgHfeeUc8l5CQ8Mw2W7ZsCYVCgUOHDuG9996rcL18zkZZWZl4zs7ODg0bNsSlS5fw9ttvV9quk5MTNm/ejKKiIrGoqSoPIqKqcEiFSIf69+8PV1dXDB8+HD/99BMyMjJw7Ngx/Pvf/8bJkycBAB988AHWr1+P9evX4/z585g7dy5Onz79zDZNTU0xa9YsfPTRR/juu+9w8eJFJCQkYN26dQAAW1tbKBQKREdH4/r168jPzwfweDOx0NBQfP311zh//jxSU1OxYcMGLFmyBADg7e0NAwMDTJgwAWfOnMH+/fsRHh6u5W+IiGorFhxEOiSTybB//368/vrrGD9+PFq1aoXRo0cjIyMDdnZ2AIBRo0Zhzpw5mDVrFrp06YIrV65g8uTJVbb76aefIigoCHPmzEGbNm0watQo5ObmAgCMjIywbNkyrFmzBiqVCsOGDQMAvPfee/j222+xceNGtGvXDm5ubti4caO4jLZOnTrYs2cPzpw5g06dOuGTTz7BwoULtfjtEFFtJhM4UEtERERaxh4OIiIi0joWHERERKR1LDiIiIhI61hwEBERkdax4CAiIiKtY8FBREREWseCg4iIiLSOBQcRERFpHQsOIiIi0joWHERERKR1LDiIiIhI6/4PuE1EKv9YGKgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- CONFUSION MATRIX ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Not Suspicious', 'Suspicious'],\n",
    "    yticklabels=['Not Suspicious', 'Suspicious']\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f613fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "fpr_pu, tpr_pu, _ = roc_curve(y_test, y_proba)\n",
    "auc_pu = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "np.savez(\"roc_bagging.npz\",\n",
    "         fpr=fpr_pu,\n",
    "         tpr=tpr_pu,\n",
    "         auc=auc_pu,\n",
    "         labels=y_test,\n",
    "         scores=y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c65f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"preds_Bagging.npy\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e24131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "fpr_pu, tpr_pu, _ = roc_curve(y_test, y_proba)\n",
    "auc_pu = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "np.savez(\"roc_bagging.npz\",\n",
    "         fpr=fpr_pu,\n",
    "         tpr=tpr_pu,\n",
    "         auc=auc_pu,\n",
    "         labels=y_test,\n",
    "         scores=y_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "028a4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna \n",
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial,X_train, y_train, X_val, y_val):\n",
    "    # Hyperparameter suggestions from Optuna\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 600)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 30)\n",
    "    max_samples = trial.suggest_int('max_samples', 500, len(X_train))\n",
    "    \n",
    "    # Initialize Bagging PU Classifier\n",
    "    pu_clf = BaggingPuClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=max_depth),\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=1.0,\n",
    "        bootstrap=True,\n",
    "        bootstrap_features=False,\n",
    "        oob_score=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train on the PU version of y_train\n",
    "    pu_clf.fit(X_train, y_train)\n",
    "    \n",
    "   \n",
    "    # Predict on the validation set\n",
    "    y_pred = pu_clf.predict(X_val)\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    val_score = f1_score(y_val, y_pred)\n",
    "    \n",
    "    return val_score\n",
    "\n",
    "def optimize_bagging_pu(X_train, y_train, X_val, y_val, n_trials):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=n_trials)\n",
    "    \n",
    "    print('Best Trial:')\n",
    "    print(study.best_trial)\n",
    "    \n",
    "    return study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63017c6",
   "metadata": {},
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 600)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "    max_samples = trial.suggest_int(\"max_samples\", 2000, 10000)\n",
    "\n",
    "    # Base estimator\n",
    "    base = DecisionTreeClassifier(max_depth=max_depth)\n",
    "\n",
    "    # PU Classifier\n",
    "    pu_clf = BaggingPuClassifier(\n",
    "        base_estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=1.0,\n",
    "        bootstrap=True,\n",
    "        bootstrap_features=False,\n",
    "        oob_score=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit model op gebalanceerde data (met oversampling)\n",
    "    pu_clf.fit(X_train.values, y_train_pu)\n",
    "\n",
    "    # Predict op testset\n",
    "    y_pred_proba = pu_clf.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "    # ROC AUC als optimalisatiedoel\n",
    "    return roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Studie starten\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)\n",
    "\n",
    "print(\"Beste trial:\")\n",
    "print(study.best_trial)\n",
    "\n",
    "# Beste model opnieuw trainen?\n",
    "best_params = study.best_params\n",
    "print(best_params)\n",
    "\n",
    "best_model = BaggingPuClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=best_params[\"max_depth\"]),\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_samples=best_params[\"max_samples\"],\n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "best_model.fit(X_train.values, y_train_pu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fe3a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-12 17:52:50,771] A new study created in memory with name: no-name-45eccbb3-05ed-4481-a6e8-afe7d21e5a58\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:01:59,223] Trial 0 finished with value: 0.16907216494845362 and parameters: {'n_estimators': 260, 'max_depth': 8, 'max_samples': 1473258}. Best is trial 0 with value: 0.16907216494845362.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:08:12,130] Trial 1 finished with value: 0.4393358876117497 and parameters: {'n_estimators': 231, 'max_depth': 25, 'max_samples': 648007}. Best is trial 1 with value: 0.4393358876117497.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:11:13,548] Trial 2 finished with value: 0.41088435374149657 and parameters: {'n_estimators': 126, 'max_depth': 13, 'max_samples': 441336}. Best is trial 1 with value: 0.4393358876117497.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:16:37,063] Trial 3 finished with value: 0.16907216494845362 and parameters: {'n_estimators': 138, 'max_depth': 8, 'max_samples': 1476017}. Best is trial 1 with value: 0.4393358876117497.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:22:09,734] Trial 4 finished with value: 0.41916167664670656 and parameters: {'n_estimators': 224, 'max_depth': 19, 'max_samples': 433164}. Best is trial 1 with value: 0.4393358876117497.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:30:42,265] Trial 5 finished with value: 0.41435562805872755 and parameters: {'n_estimators': 186, 'max_depth': 15, 'max_samples': 1638348}. Best is trial 1 with value: 0.4393358876117497.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:38:31,739] Trial 6 finished with value: 0.4397163120567376 and parameters: {'n_estimators': 175, 'max_depth': 30, 'max_samples': 1364917}. Best is trial 6 with value: 0.4397163120567376.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:46:48,701] Trial 7 finished with value: 0.4366812227074236 and parameters: {'n_estimators': 185, 'max_depth': 26, 'max_samples': 1468214}. Best is trial 6 with value: 0.4397163120567376.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:51:11,797] Trial 8 finished with value: 0.4089219330855019 and parameters: {'n_estimators': 207, 'max_depth': 14, 'max_samples': 359094}. Best is trial 6 with value: 0.4397163120567376.\n",
      "/Users/alixvanhees/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2025-05-12 18:54:12,607] Trial 9 finished with value: 0.21759259259259262 and parameters: {'n_estimators': 229, 'max_depth': 6, 'max_samples': 109082}. Best is trial 6 with value: 0.4397163120567376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial:\n",
      "FrozenTrial(number=6, state=TrialState.COMPLETE, values=[0.4397163120567376], datetime_start=datetime.datetime(2025, 5, 12, 18, 30, 42, 270882), datetime_complete=datetime.datetime(2025, 5, 12, 18, 38, 31, 738574), params={'n_estimators': 175, 'max_depth': 30, 'max_samples': 1364917}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=300, log=False, low=50, step=1), 'max_depth': IntDistribution(high=30, log=False, low=5, step=1), 'max_samples': IntDistribution(high=1680000, log=False, low=500, step=1)}, trial_id=6, value=None)\n"
     ]
    }
   ],
   "source": [
    "# Start Optuna Optimization\n",
    "best_pu_clf = optimize_bagging_pu(X_train, y_train, X_val, y_val, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3789133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in ./opt/anaconda3/lib/python3.8/site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./opt/anaconda3/lib/python3.8/site-packages (from optuna) (1.14.1)\n",
      "Requirement already satisfied: colorlog in ./opt/anaconda3/lib/python3.8/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.8/site-packages (from optuna) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./opt/anaconda3/lib/python3.8/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in ./opt/anaconda3/lib/python3.8/site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.8/site-packages (from optuna) (4.66.2)\n",
      "Requirement already satisfied: PyYAML in ./opt/anaconda3/lib/python3.8/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in ./opt/anaconda3/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: importlib-metadata in ./opt/anaconda3/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (7.0.1)\n",
      "Requirement already satisfied: importlib-resources in ./opt/anaconda3/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (6.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4 in ./opt/anaconda3/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./opt/anaconda3/lib/python3.8/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./opt/anaconda3/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4b8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
